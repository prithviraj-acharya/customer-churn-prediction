


## REQUIRED LIBRARIES

# For data wrangling
import pandas as pd
import numpy as np

# For visualization
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
from plotly.offline import init_notebook_mode

# For preprocessing and modeling
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestClassifier

#Model building
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score




init_notebook_mode(connected=True)







df = pd.read_csv('customer_churn_dataset.csv')
df.head(2)


df.shape


df.info()


df.isnull().sum()


churned_out_color = '#B71C1C'
active_customers_color = '#00BFA5'



# Data Visualization and Exploration 
# Prepare the data
labels = ['Churned Out', 'Active Customers']
sizes = [df.Churn[df['Churn'] == 1].count(), df.Churn[df['Churn'] == 0].count()]
print(sizes)

# Create the pie chart
fig = px.pie(
    names=labels,
    values=sizes,
    title="Proportion of Customers Churned out and Active Customers",
    hole=0.0,  # For a standard pie chart; set hole=0.5 for a donut chart
)

# Optional: Tuning visual appearance
fig.update_traces(
    pull=[0, 0.05],  # Pulls the 'Retained' slice out slightly, similar to "explode"
    textinfo='percent+label',  # Show percentage and label together
    hoverinfo='label+percent+value',  # Hover information
    marker=dict(line=dict(color='black', width=0.5),colors=[churned_out_color, active_customers_color]),  # Customize marker line
)

# Adjust the layout to set the width and height
fig.update_layout(
    width=800,  # Set desired width (e.g., 600 pixels)
    height=500  # Set desired height (e.g., 400 pixels)
)


# Show the chart
fig.show()



# Prepare data for analysis and exploration
# - Create a copy of the original DataFrame for exploratory data analysis (EDA)
# - Remove the 'customerID' column as it is irrelevant for modeling
# - Map categorical values in 'Churn' and 'SeniorCitizen' columns to more meaningful labels
#   for better readability and interpretation

df_copy = df.copy()

# Drop the customerID column
if 'customerID' in df.columns:
    df = df.drop(columns=['customerID'])

# Drop the customerID column
if 'customerID' in df_copy.columns:
    df_copy = df_copy.drop(columns=['customerID'])

# Map the Churn column to the desired labels in the copy
df_copy['Churn'] = df_copy['Churn'].map({0: 'Active Customers', 1: 'Churned Out'})
df_copy['SeniorCitizen'] = df_copy['SeniorCitizen'].map({0: 'Non-Senior Citizen', 1: 'Senior Citizen'})


#Gender
fig = px.histogram(df_copy,
                   x='gender',
                   color='Churn',
                   title='Churn Rate by Gender',
                   barmode='group',
                   color_discrete_sequence=[churned_out_color,active_customers_color],)

fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)
fig.show()


#SeniorCitizen
fig = px.histogram(df_copy,
                   x='SeniorCitizen',
                   color='Churn',
                   title='Churn Rate by Senior Citizen',
                   barmode='group',
                   color_discrete_sequence=[churned_out_color,active_customers_color],)

fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)
fig.show()




#Partner
fig = px.histogram(df_copy,
                   x='Partner',
                   color='Churn',
                   title='Churn Rate by Partner',
                   barmode='group',
                   color_discrete_sequence=[churned_out_color,active_customers_color],)

fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)
fig.show()


#Dependents
fig = px.histogram(df_copy,
                   x='Dependents',
                   color='Churn',
                   title='Churn Rate by Dependents',
                   barmode='group',
                   color_discrete_sequence=[churned_out_color,active_customers_color],)

fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)
fig.show()


#PhoneService
fig = px.histogram(df_copy,
                   x='PhoneService',
                   color='Churn',
                   title='Churn Rate by PhoneService',
                   barmode='group',
                   color_discrete_sequence=[churned_out_color,active_customers_color],)

fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)
fig.show()


#MultipleLines
fig = px.histogram(df_copy,
                   x='MultipleLines',
                   color='Churn',
                   title='Churn Rate by MultipleLines',
                   barmode='group',
                   color_discrete_sequence=[churned_out_color,active_customers_color],)

fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)
fig.show()


#InternetService
fig = px.histogram(df_copy,
                   x='InternetService',
                   color='Churn',
                   title='Churn Rate by InternetService',
                   barmode='group',
                   color_discrete_sequence=[churned_out_color,active_customers_color],)

fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)
fig.show()


#OnlineSecurity
fig = px.histogram(df_copy,
                   x='OnlineSecurity',
                   color='Churn',
                   title='Churn Rate by OnlineSecurity',
                   barmode='group',
                   color_discrete_sequence=[churned_out_color,active_customers_color],)

fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)
fig.show()


#OnlineBackup
fig = px.histogram(df_copy,
                   x='OnlineBackup',
                   color='Churn',
                   title='Churn Rate by OnlineBackup',
                   barmode='group',
                   color_discrete_sequence=[churned_out_color,active_customers_color],)

fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)
fig.show()


#tenure
# Group and aggregate data
grouped_data = df_copy.groupby(['tenure', 'Churn']).size().reset_index(name='Customer Count')

# Create the line chart
fig = px.line(
    grouped_data,
    x='tenure',
    y='Customer Count',
    color='Churn',
    title='Churn Rate by Tenure',
    color_discrete_sequence=[active_customers_color,churned_out_color]
)

# Update layout for better labels
fig.update_layout(
    xaxis_title='Tenure',
    yaxis_title='Customer Count',
    legend_title='Churn Status',

)

# Show the figure
fig.show()






# Drop rows with missing values
df_copy = df_copy.dropna()

# Encode categorical variables
label_encoders = {}
for column in df_copy.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df_copy[column] = le.fit_transform(df_copy[column])
    label_encoders[column] = le

# Compute the correlation matrix
correlation_matrix = df_copy.corr()

# Plot the heatmap
plt.figure(figsize=(10, 5))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix')
plt.show()








# Preprocess the data
df_copy = df_copy.dropna()  # Drop rows with missing values
label_encoders = {}
for column in df_copy.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df_copy[column] = le.fit_transform(df_copy[column])
    label_encoders[column] = le

# Split the data into features and target
X = df_copy.drop('Churn', axis=1)
y = df_copy['Churn']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a random forest classifier
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train, y_train)

# Get feature importance
feature_importance = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)

# Print feature importance
print(feature_importance)






#Creating baseline models
# Preprocess the data (assuming df_copy is already preprocessed and ready)
# Split the data into features and target
x = df_copy.drop('Churn', axis=1)
y = df_copy['Churn']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Train a Decision Tree classifier
dt_clf = DecisionTreeClassifier(random_state=42, criterion='entropy', max_depth=5, )
dt_clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = dt_clf.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1_score_baseline_dt = f1_score(y_test, y_pred)
print(f'Accuracy of the DecisionTreeClassifier model: {accuracy:.3f}')
print(f'Precision of the DecisionTreeClassifier model: {precision:.3f}')
print(f'Recall of the DecisionTreeClassifier model: {recall:.3f}')
print(f'F1 Score of the DecisionTreeClassifier model: {f1_score_baseline_dt:.3f}')



#Creating baseline models

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Preprocess the data (assuming df_copy is already preprocessed and ready)
# Split the data into features and target
x = df_copy.drop('Churn', axis=1)
y = df_copy['Churn']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Train a Logistic Regression classifier
lr_clf = LogisticRegression(random_state=42, max_iter=500)
lr_clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = lr_clf.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1_score_baseline_lr = f1_score(y_test, y_pred)
print(f'Accuracy of the Logistic Regression model: {accuracy:.3f}')
print(f'Precision of the Logistic Regression model: {precision:.3f}')
print(f'Recall of the Logistic Regression model: {recall:.3f}')
print(f'F1 Score of the Logistic Regression model: {f1_score_baseline_lr:.3f}')






# Filling up the missing values

#Gender
missing_gender_percent = df['gender'].isnull().sum() / len(df) * 100
print(f"Missing Gender Values: {missing_gender_percent:.2f}%")
df.loc[df['gender'].isnull(), 'gender'] = "Unknown"

#Senior Citizen
missing_senior_citizen_percent = df['SeniorCitizen'].isnull().sum() / len(df) * 100
print(f"Missing SeniorCitizen Values: {missing_senior_citizen_percent:.2f}%")
senior_dist = df['SeniorCitizen'].value_counts(normalize=True)
df.loc[df['SeniorCitizen'].isnull(), 'SeniorCitizen'] = np.random.choice([0.0, 1.0], p=senior_dist.values)

#Partner
missing_partner = df['Partner'].isnull().sum() / len(df) * 100
print(f"Missing Partner Values: {missing_partner:.2f}%")
partner_dist = df['Partner'].value_counts(normalize=True)
df.loc[df['Partner'].isnull(), 'Partner'] = np.random.choice(['Yes', 'No'], p=partner_dist.values)

#Dependents
missing_dependents = df['Dependents'].isnull().sum() / len(df) * 100
print(f"Missing Dependents Values: {missing_dependents:.2f}%")
dependent_dist = df['Dependents'].value_counts(normalize=True)
df.loc[df['Dependents'].isnull(), 'Dependents'] = np.random.choice(['Yes', 'No'], p=dependent_dist.values)

#Tenure
missing_tenure = df['tenure'].isnull().sum() / len(df) * 100
print(f"Missing Tenure Values: {missing_tenure:.2f}%")
df.loc[df['tenure'].isnull(), 'tenure'] = df['tenure'].median()

#Phone Service
missing_phone_service = df['PhoneService'].isnull().sum() / len(df) * 100
print(f"Missing PhoneService Values: {missing_phone_service:.2f}%")
phone_service_dist = df['PhoneService'].value_counts(normalize=True)
df.loc[df['PhoneService'].isnull(), 'PhoneService'] = np.random.choice(['Yes', 'No'], p=phone_service_dist.values)

#Multiple Lines
missing_multiple_lines = df['MultipleLines'].isnull().sum() / len(df) * 100
print(f"Missing MultipleLines Values: {missing_multiple_lines:.2f}%")
multiple_lines_dist = df['MultipleLines'].value_counts(normalize=True)
df.loc[df['MultipleLines'].isnull(), 'MultipleLines'] = np.random.choice(multiple_lines_dist.index, p=multiple_lines_dist.values)

#Internet Service
missing_internet_service = df['InternetService'].isnull().sum() / len(df) * 100
print(f"Missing InternetService Values: {missing_internet_service:.2f}%")
internet_service_dist = df['InternetService'].value_counts(normalize=True)
df.loc[df['InternetService'].isnull(), 'InternetService'] = np.random.choice(internet_service_dist.index, p=internet_service_dist.values)

#Online Security
missing_online_security = df['OnlineSecurity'].isnull().sum() / len(df) * 100
print(f"Missing OnlineSecurity Values: {missing_online_security:.2f}%")
online_security_dist = df['OnlineSecurity'].value_counts(normalize=True)
df.loc[df['OnlineSecurity'].isnull(), 'OnlineSecurity'] = np.random.choice(online_security_dist.index, p=online_security_dist.values)

#Online Backup
missing_online_backup = df['OnlineBackup'].isnull().sum() / len(df) * 100
print(f"Missing OnlineBackup Values: {missing_online_backup:.2f}%")
online_backup_dist = df['OnlineBackup'].value_counts(normalize=True)
df.loc[df['OnlineBackup'].isnull(), 'OnlineBackup'] = np.random.choice(online_backup_dist.index, p=online_backup_dist.values)






df.isnull().sum()



df.info()


##Encoding the data

# Create a LabelEncoder object for binary features
df.head()
# List of binary columns (for Label Encoding)
binary_cols = ['SeniorCitizen', 'Partner', 'Dependents', 'PhoneService']

# Apply Label Encoding to binary features
le = LabelEncoder()
for col in binary_cols:
    df[col] = le.fit_transform(df[col])

# List of categorical columns (for One-Hot Encoding)
categorical_cols = ['gender', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup']

# Apply One-Hot Encoding
df_preprocessed = pd.get_dummies(df, columns=categorical_cols, drop_first=False, dtype='int')


# Initialize MinMaxScaler
scaler = MinMaxScaler()

# Apply MinMaxScaler to the 'tenure' field and create a new column 'scaled_tenure'
df_preprocessed['scaled_tenure'] = scaler.fit_transform(df[['tenure']])






# Print confirmation
print("DataFrame `df_preprocessed` is ready for model training!")
df_preprocessed.head()


df_preprocessed.describe()



import plotly.express as px

# Drop the 'tenure' column
filtered_df = df_preprocessed.drop(columns=['tenure'])

# Convert DataFrame to long format for Plotly
df_melted = filtered_df.melt(var_name='Feature', value_name='Value')

# Create an interactive box plot with thicker elements
fig = px.box(
    df_melted, 
    x='Value', 
    y='Feature', 
    title="Box Plot of Features",
    color='Feature',  # Different colors for each feature
    color_discrete_sequence=px.colors.qualitative.Prism  # Color palette
)

# Increase thickness of box elements
fig.update_traces(
    boxmean=True,  # Show mean as a line inside the box
    marker=dict(size=6),  # Make outlier points bigger
    line=dict(width=3)  # Make box plot lines thicker
)

# Improve layout
fig.update_layout(
    xaxis_title="Value Distribution",
    yaxis_title="Features",
    width=900,
    height=500,
    font=dict(family="Arial, sans-serif", size=12, color="black"),
    margin=dict(l=100, r=50, t=50, b=50)  # Adjust margins
)

fig.show()






#Checking model building with manual tuning of hyperparameters - Decision Tree

# Decision Tree Classifier - Test Size = 0.2
x_dt = df_preprocessed.drop(['Churn', 'scaled_tenure'], axis=1)
y_dt = df_preprocessed['Churn']

# Split the data with test_size = 0.2
x_train_dt, x_test_dt, y_train_dt, y_test_dt = train_test_split(
    x_dt, y_dt, test_size=0.2, random_state=42
)

# Initialize and fit the Decision Tree Classifier with the given hyperparameters (manual tuning)
dt_clf = DecisionTreeClassifier(
    random_state=42,
    criterion='entropy',
    max_depth=7,
    min_samples_leaf=1,
    min_samples_split=2
)
dt_clf.fit(x_train_dt, y_train_dt)

# Make predictions
y_pred_dt = dt_clf.predict(x_test_dt)

# Evaluate performance
accuracy = accuracy_score(y_test_dt, y_pred_dt)
precision = precision_score(y_test_dt, y_pred_dt, pos_label=1)
recall = recall_score(y_test_dt, y_pred_dt, pos_label=1)
f1 = f1_score(y_test_dt, y_pred_dt, pos_label=1)
auc_roc = roc_auc_score(y_test_dt, y_pred_dt)

# Display results
print("\nResults of Decision Tree Classifier with Test Size = 0.2:")
print(f"Accuracy: {accuracy:.3f}")
print(f"Precision: {precision:.3f}")
print(f"Recall: {recall:.3f}")
print(f"F1-Score: {f1:.3f}")
print(f"AUC-ROC: {auc_roc:.3f}")



#Finding the best hyperparameters for the Decision tree with Grid Search CV

x_dt = df_preprocessed.drop(['Churn','scaled_tenure'], axis=1)
y_dt = df_preprocessed['Churn']

# Split data into training and testing sets
x_train_dt, x_test_dt, y_train_dt, y_test_dt = train_test_split(x_dt, y_dt, test_size=0.2, random_state=42)

# Define the refined parameter grid
param_grid = {
    'max_depth': [1, 2, 3, 5, 7],  # Avoiding 'None' since deep trees overfit
    'criterion': ['gini', 'entropy'],
    'min_samples_split': [2, 3, 5],
    'min_samples_leaf': [1, 2, 3]
}

# Initialize GridSearchCV with 5-fold cross-validation
grid_search_dt = GridSearchCV(
    estimator=DecisionTreeClassifier(random_state=42),
    param_grid=param_grid,
    scoring='f1',
    cv=5,
    verbose=1,
    n_jobs=-1
)

# Perform the grid search
grid_search_dt.fit(x_train_dt, y_train_dt)

# Retrieve the best model
best_clf = grid_search_dt.best_estimator_

# Make predictions on the test set using the best model
y_pred_dt = best_clf.predict(x_test_dt)

# Evaluate the best model
accuracy_dt = accuracy_score(y_test_dt, y_pred_dt)
precision_dt = precision_score(y_test_dt, y_pred_dt)
recall_dt = recall_score(y_test_dt, y_pred_dt)
f1_score_dt = f1_score(y_test_dt, y_pred_dt)

# Print the results
print("Best Parameters for Decision Tree Classifier:", grid_search_dt.best_params_)
print(f'Accuracy: {accuracy_dt:.3f}')
print(f'Precision: {precision_dt:.3f}')
print(f'Recall: {recall_dt:.3f}')
print(f'F1 Score: {f1_score_dt:.3f}')



# Decision Tree Classifier
x_dt = df_preprocessed.drop(['Churn','scaled_tenure'], axis=1)
y_dt = df_preprocessed['Churn']

# Define test sizes to evaluate
test_sizes = [0.1, 0.2, 0.3, 0.4]

# Store results for each test size
results_dt = []

for test_size in test_sizes:
    # print(f"\nTesting with test_size = {test_size}")

    # Split the data
    x_train_dt, x_test_dt, y_train_dt, y_test_dt = train_test_split(
        x_dt, y_dt, test_size=test_size, random_state=42
    )

    # Initialize and fit the Decision Tree Classifier with the given hyperparameters
    dt_clf = DecisionTreeClassifier(**grid_search_dt.best_params_)
    dt_clf.fit(x_train_dt, y_train_dt)

    # Make predictions
    y_pred_dt = dt_clf.predict(x_test_dt)

    # Evaluate performance
    accuracy = accuracy_score(y_test_dt, y_pred_dt)
    precision = precision_score(y_test_dt, y_pred_dt, pos_label=1)  # Handle undefined precision
    recall = recall_score(y_test_dt, y_pred_dt, pos_label=1)
    f1 = f1_score(y_test_dt, y_pred_dt, pos_label=1)
    auc_roc_lr = roc_auc_score(y_test_dt, y_pred_dt)


    # Store the results
    results_dt.append((test_size, accuracy, precision, recall, f1, auc_roc_lr ))

    # print(f"Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}")

# Display all results at the end
print("\nSummary of Results of Decision Tree Classifier:")
for i, (test_size, accuracy, precision, recall, f1, auc_roc_lr) in enumerate(results_dt):
    if i == 1:  # Highlight the second record
        print(
            f"\033[1mTest Size: {test_size:.2f} | Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}, AUC-ROC: {auc_roc_lr:.3f}\033[0m")
    else:
        print(
            f"Test Size: {test_size:.2f} | Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}, AUC-ROC: {auc_roc_lr:.3f}")






#Manual tuning of hyperparameters for Decision Logistic Regression

# Logistic Regression Classifier - Test Size = 0.2
x_lr = df_preprocessed.drop(['Churn', 'tenure'], axis=1)
y_lr = df_preprocessed['Churn']

# Split the data with test_size = 0.2
x_train_lr, x_test_lr, y_train_lr, y_test_lr = train_test_split(
    x_lr, y_lr, test_size=0.2, random_state=42
)

# Initialize and fit the Logistic Regression model
model = LogisticRegression(
    random_state=42,
    C=0.01,
    l1_ratio=0.5,
    max_iter=200,
    penalty='elasticnet',
    solver='saga'
)

model.fit(x_train_lr, y_train_lr)

# Make predictions
y_pred = model.predict(x_test_lr)

# Evaluate performance
accuracy = accuracy_score(y_test_lr, y_pred)
precision = precision_score(y_test_lr, y_pred, zero_division=1)  # Handle undefined precision
recall = recall_score(y_test_lr, y_pred, zero_division=1)
f1 = f1_score(y_test_lr, y_pred, zero_division=1)
auc_roc = roc_auc_score(y_test_lr, y_pred)

# Display results
print("\nResults of Logistic Regression Classifier with Test Size = 0.2 & Manual tuning the hyperparameters")
print(f"Accuracy: {accuracy:.3f}")
print(f"Precision: {precision:.3f}")
print(f"Recall: {recall:.3f}")
print(f"F1-Score: {f1:.3f}")
print(f"AUC-ROC: {auc_roc:.3f}")





#Finding the best hyperparameters for the Logistic Regression

# Finding the best hyperparameters for Logistic Regression
x_lr = df_preprocessed.drop(['Churn', 'tenure'], axis=1)
y_lr = df_preprocessed['Churn']

# Split data into training and testing sets
x_train_lr, x_test_lr, y_train_lr, y_test_lr = train_test_split(x_lr, y_lr, test_size=0.2, random_state=42)

# Define the parameter grid for Logistic Regression
# Define the parameter grid
param_grid_lr = [
    {'penalty': ['l1'], 'C': [0.01, 0.1, 1, 10], 'solver': ['liblinear'], 'max_iter': [20, 50, 100, 200]},
    {'penalty': ['l2'], 'C': [0.01, 0.1, 1, 10], 'solver': ['liblinear', 'saga'], 'max_iter': [20, 50, 100, 200]},
    {'penalty': ['elasticnet'], 'C': [0.01, 0.1, 1, 10], 'solver': ['saga'], 'l1_ratio': [0.5], 'max_iter': [20, 50, 100, 200]}
]


# Initialize GridSearchCV
grid_search_lr = GridSearchCV(
    estimator=LogisticRegression(random_state=42),
    param_grid=param_grid_lr,
    scoring='f1',
    cv=5,
    verbose=1,
    n_jobs=-1
)

# Perform the grid search
grid_search_lr.fit(x_train_lr, y_train_lr)

# Retrieve the best model from the search
best_lr_clf = grid_search_lr.best_estimator_

# Make predictions on the test set using the best model
y_pred_lr = best_lr_clf.predict(x_test_lr)

# Evaluate the best model
accuracy_lr = accuracy_score(y_test_lr, y_pred_lr)
precision_lr = precision_score(y_test_lr, y_pred_lr)
recall_lr = recall_score(y_test_lr, y_pred_lr)
f1_score_lr = f1_score(y_test_lr, y_pred_lr)

# Print the results
print("Best Parameters for Logistic Regression Classifier:", grid_search_lr.best_params_)
print(f'Accuracy: {accuracy_lr:.3f}')
print(f'Precision: {precision_lr:.3f}')
print(f'Recall: {recall_lr:.3f}')
print(f'F1 Score: {f1_score_lr:.3f}')




#Logistic Regression Classifier
x_lr = df_preprocessed.drop(['Churn','tenure'], axis=1)
y_lr = df_preprocessed['Churn']

# Define possible test sizes
test_sizes = [0.1, 0.2, 0.3, 0.4]

# Store results for each test size
results_lr = []

for test_size in test_sizes:
    # print(f"\nTesting with test_size = {test_size}")

    # Split the data
    x_train_lr, x_test_lr, y_train_lr, y_test_lr = train_test_split(
        x_lr, y_lr, test_size=test_size, random_state=42
    )

    # Initialize and fit the Logistic Regression model
    model = LogisticRegression(**grid_search_lr.best_params_)

    model.fit(x_train_lr, y_train_lr)

    # Make predictions
    y_pred = model.predict(x_test_lr)

    # Evaluate performance
    accuracy = accuracy_score(y_test_lr, y_pred)
    precision = precision_score(y_test_lr, y_pred, zero_division=1)  # Handle undefined precision
    recall = recall_score(y_test_lr, y_pred, zero_division=1)
    f1 = f1_score(y_test_lr, y_pred, zero_division=1)
    auc_roc = roc_auc_score(y_test_lr, y_pred)

    # Store the results
    results_lr.append((test_size, accuracy, precision, recall, f1, auc_roc))

    # print(f"Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}")

# Display all results at the end
print("\nSummary of Results of Logistic Regression Classifier:")
for i, (test_size, accuracy, precision, recall, f1, auc_roc) in enumerate(results_lr):
    if i == 1:  # Highlight the second record
        print(
            f"\033[1mTest Size: {test_size:.2f} | Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}, AUC-ROC: {auc_roc:.3f}\033[0m")
    else:
        print(
            f"Test Size: {test_size:.2f} | Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}, AUC-ROC: {auc_roc:.3f}")












# K-Nearest Neighbors Classifier

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Define features and target
x_knn = df_preprocessed.drop(['Churn', 'tenure'], axis=1)
y_knn = df_preprocessed['Churn']

# Define possible test sizes
test_sizes = [0.1, 0.2, 0.3, 0.4]

# Store results for each test size
results_knn = []

for test_size in test_sizes:
    # Split the data
    x_train_knn, x_test_knn, y_train_knn, y_test_knn = train_test_split(
        x_knn, y_knn, test_size=test_size, random_state=42
    )

    # Initialize the KNN model with the specified hyperparameters (Hyper parameters are taken from the grid search)
    knn_model = KNeighborsClassifier(
        metric='euclidean',
        n_neighbors=5,
        weights='distance'
    )

    # Fit the model
    knn_model.fit(x_train_knn, y_train_knn)

    # Make predictions
    y_pred_knn = knn_model.predict(x_test_knn)

    # Evaluate performance
    accuracy = accuracy_score(y_test_knn, y_pred_knn)
    precision = precision_score(y_test_knn, y_pred_knn, zero_division=1)
    recall = recall_score(y_test_knn, y_pred_knn, zero_division=1)
    f1 = f1_score(y_test_knn, y_pred_knn, zero_division=1)
    auc_roc = roc_auc_score(y_test_knn, y_pred_knn)

    # Store the results
    results_knn.append((test_size, accuracy, precision, recall, f1, auc_roc))

# Display all results at the end
print("\nSummary of Results for K-Nearest Neighbors Classifier:")
for i, (test_size, accuracy, precision, recall, f1, auc_roc) in enumerate(results_knn):
    if i == 1:  # Highlight the second record
        print(
            f"\033[1mTest Size: {test_size:.2f} | Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}, AUC-ROC: {auc_roc:.3f}\033[0m")
    else:
        print(
            f"Test Size: {test_size:.2f} | Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}, AUC-ROC: {auc_roc:.3f}")





# Random Forest Classifier

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Feature matrix and target variable
x_rf = df_preprocessed.drop(['Churn', 'tenure'], axis=1)
y_rf = df_preprocessed['Churn']

# Define possible test sizes
test_sizes = [0.1, 0.2, 0.3, 0.4]

# Store results for each test size
results_rf = []

for test_size in test_sizes:
    # Split the data
    x_train_rf, x_test_rf, y_train_rf, y_test_rf = train_test_split(
        x_rf, y_rf, test_size=test_size, random_state=42
    )

    # Initialize Random Forest model with specified hyperparameters
    rf_model = RandomForestClassifier(
        n_estimators=100,  # Number of trees in the forest
        max_depth=None,  # Maximum depth of the tree (None means nodes expand until all leaves are pure)
        random_state=42,  # Random seed for reproducibility
        bootstrap=True,  # Bagging enabled
    )

    # Fit the model on the training data
    rf_model.fit(x_train_rf, y_train_rf)

    # Make predictions on the test data
    y_pred_rf = rf_model.predict(x_test_rf)

    # Evaluate performance
    accuracy = accuracy_score(y_test_rf, y_pred_rf)
    precision = precision_score(y_test_rf, y_pred_rf, zero_division=1)
    recall = recall_score(y_test_rf, y_pred_rf, zero_division=1)
    f1 = f1_score(y_test_rf, y_pred_rf, zero_division=1)
    auc_roc = roc_auc_score(y_test_rf, y_pred_rf)

    # Store results
    results_rf.append((test_size, accuracy, precision, recall, f1, auc_roc))

# Display all results at the end
print("\nSummary of Results for Random Forest Classifier (Bagging):")
for i, (test_size, accuracy, precision, recall, f1, auc_roc) in enumerate(results_rf):
    if i == 1:  # Highlight the second record
        print(
            f"\033[1mTest Size: {test_size:.2f} | Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}, AUC-ROC: {auc_roc:.3f}\033[0m"
        )
    else:
        print(
            f"Test Size: {test_size:.2f} | Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}, AUC-ROC: {auc_roc:.3f}"
        )



import pandas as pd
import plotly.express as px

# Data
models = ['Decision Tree', 'Logistic Regression', 'KNN', 'Random Forest']
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']

# Extract the second (index 1) results for each model and round to 3 decimal places
data = {
    'Decision Tree': [round(metric, 3) for metric in results_dt[1][1:]],  # Skip the test size (1st item)
    'Logistic Regression': [round(metric, 3) for metric in results_lr[1][1:]],  # Skip the test size
    'KNN': [round(metric, 3) for metric in results_knn[1][1:]],  # Skip the test size
    'Random Forest': [round(metric, 3) for metric in results_rf[1][1:]]  # Skip the test size
}

# Print the created data dictionary
# print("Extracted Data (Rounded to 3 Decimal Places):")
# print(data)

# Convert to DataFrame
df_models = pd.DataFrame(data, index=metrics)

# Transform DataFrame into long format for Plotly
df_melted = df_models.reset_index().melt(id_vars='index', var_name='Model', value_name='Score')
df_melted.rename(columns={'index': 'Metric'}, inplace=True)

# Plot using Plotly
fig = px.histogram(
    df_melted,
    x='Metric',  # Metrics on the x-axis
    y='Score',  # Scores on the y-axis
    color='Model',  # Grouped by models
    barmode='group',  # Bars grouped side-by-side
    title='Model Performance Comparison',  # Title of the chart
    color_discrete_sequence=px.colors.qualitative.Prism  # Define color palette
)

# Customize layout
fig.update_layout(
    xaxis_title='Evaluation Metrics',
    yaxis_title='Score',
    width=1000,
    height=500,
    legend_title='Models'
)

# Show the interactive plot
fig.show()







