## REQUIRED LIBRARIES
# For data wrangling
import pandas as pd
import numpy as np

# For visualization
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
from plotly.offline import init_notebook_mode

# For preprocessing and modeling
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestClassifier

#Model building
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score




init_notebook_mode(connected=True)




df = pd.read_csv('customer_churn_dataset.csv')
df.head(2)


df.shape


df.info()


df.isnull().sum()





churned_out_color = '#B71C1C'
active_customers_color = '#00BFA5'


# Prepare the data
labels = ['Churned Out', 'Active Customers']
sizes = [df.Churn[df['Churn'] == 1].count(), df.Churn[df['Churn'] == 0].count()]
print(sizes)

# Create the pie chart
fig = px.pie(
    names=labels,
    values=sizes,
    title="Proportion of Customers Churned out and Active Customers",
    hole=0.0,  # For a standard pie chart; set hole=0.5 for a donut chart
)

# Optional: Tuning visual appearance
fig.update_traces(
    pull=[0, 0.05],  # Pulls the 'Retained' slice out slightly, similar to "explode"
    textinfo='percent+label',  # Show percentage and label together
    hoverinfo='label+percent+value',  # Hover information
    marker=dict(line=dict(color='black', width=0.5),colors=[churned_out_color, active_customers_color]),  # Customize marker line
)

# Adjust the layout to set the width and height
fig.update_layout(
    width=800,  # Set desired width (e.g., 600 pixels)
    height=500  # Set desired height (e.g., 400 pixels)
)


# Show the chart
fig.show(renderer='iframe')



# Create a copy of the original DataFrame
df_copy = df.copy()

# Drop the customerID column
if 'customerID' in df.columns:
    df = df.drop(columns=['customerID'])

# Drop the customerID column
if 'customerID' in df_copy.columns:
    df_copy = df_copy.drop(columns=['customerID'])

# Map the Churn column to the desired labels in the copy
df_copy['Churn'] = df_copy['Churn'].map({0: 'Active Customers', 1: 'Churned Out'})
df_copy['SeniorCitizen'] = df_copy['SeniorCitizen'].map({0: 'Non-Senior Citizen', 1: 'Senior Citizen'})


#Gender
fig = px.histogram(df_copy,
                   x='gender',
                   color='Churn',
                   title='Churn Rate by Gender',
                   barmode='group',
                   color_discrete_sequence=[churned_out_color,active_customers_color],)

fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)
fig.show(renderer='iframe')


#SeniorCitizen
fig = px.histogram(df_copy,
                   x='SeniorCitizen',
                   color='Churn',
                   title='Churn Rate by Senior Citizen',
                   barmode='group',
                   color_discrete_sequence=[churned_out_color,active_customers_color],)

fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)
fig.show(renderer='iframe')




#Partner
fig = px.histogram(df_copy,
                   x='Partner',
                   color='Churn',
                   title='Churn Rate by Partner',
                   barmode='group',
                   color_discrete_sequence=[churned_out_color,active_customers_color],)

fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)
fig.show(renderer='iframe')


#Dependents
fig = px.histogram(df_copy,
                   x='Dependents',
                   color='Churn',
                   title='Churn Rate by Dependents',
                   barmode='group',
                   color_discrete_sequence=[churned_out_color,active_customers_color],)

fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)
fig.show(renderer='iframe')


#PhoneService
fig = px.histogram(df_copy,
                   x='PhoneService',
                   color='Churn',
                   title='Churn Rate by PhoneService',
                   barmode='group',
                   color_discrete_sequence=[churned_out_color,active_customers_color],)

fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)
fig.show(renderer='iframe')


#MultipleLines
fig = px.histogram(df_copy,
                   x='MultipleLines',
                   color='Churn',
                   title='Churn Rate by MultipleLines',
                   barmode='group',
                   color_discrete_sequence=[churned_out_color,active_customers_color],)

fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)
fig.show(renderer='iframe')


#InternetService
fig = px.histogram(df_copy,
                   x='InternetService',
                   color='Churn',
                   title='Churn Rate by InternetService',
                   barmode='group',
                   color_discrete_sequence=[churned_out_color,active_customers_color],)

fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)
fig.show(renderer='iframe')


#OnlineSecurity
fig = px.histogram(df_copy,
                   x='OnlineSecurity',
                   color='Churn',
                   title='Churn Rate by OnlineSecurity',
                   barmode='group',
                   color_discrete_sequence=[churned_out_color,active_customers_color],)

fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)
fig.show(renderer='iframe')


#OnlineBackup
fig = px.histogram(df_copy,
                   x='OnlineBackup',
                   color='Churn',
                   title='Churn Rate by OnlineBackup',
                   barmode='group',
                   color_discrete_sequence=[churned_out_color,active_customers_color],)

fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)
fig.show(renderer='iframe')


#tenure
# Group and aggregate data
grouped_data = df_copy.groupby(['tenure', 'Churn']).size().reset_index(name='Customer Count')

# Create the line chart
fig = px.line(
    grouped_data,
    x='tenure',
    y='Customer Count',
    color='Churn',
    title='Churn Rate by Tenure',
    color_discrete_sequence=[active_customers_color,churned_out_color]
)

# Update layout for better labels
fig.update_layout(
    xaxis_title='Tenure',
    yaxis_title='Customer Count',
    legend_title='Churn Status',

)

# Show the figure
fig.show(renderer='iframe')



# Drop rows with missing values
df_copy = df_copy.dropna()

# Encode categorical variables
label_encoders = {}
for column in df_copy.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df_copy[column] = le.fit_transform(df_copy[column])
    label_encoders[column] = le

# Compute the correlation matrix
correlation_matrix = df_copy.corr()

# Plot the heatmap
plt.figure(figsize=(10, 5))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix')
plt.show()





# Preprocess the data
df_copy = df_copy.dropna()  # Drop rows with missing values
label_encoders = {}
for column in df_copy.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df_copy[column] = le.fit_transform(df_copy[column])
    label_encoders[column] = le

# Split the data into features and target
X = df_copy.drop('Churn', axis=1)
y = df_copy['Churn']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a random forest classifier
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train, y_train)

# Get feature importance
feature_importance = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)

# Print feature importance
print(feature_importance)



#Creating baseline models
# Preprocess the data (assuming df_copy is already preprocessed and ready)
# Split the data into features and target
x = df_copy.drop('Churn', axis=1)
y = df_copy['Churn']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Train a Decision Tree classifier
dt_clf = DecisionTreeClassifier(random_state=42, criterion='entropy', max_depth=5, )
dt_clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = dt_clf.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1_score_baseline_dt = f1_score(y_test, y_pred)
print(f'Accuracy of the DecisionTreeClassifier model: {accuracy:.3f}')
print(f'Precision of the DecisionTreeClassifier model: {precision:.3f}')
print(f'Recall of the DecisionTreeClassifier model: {recall:.3f}')
print(f'F1 Score of the DecisionTreeClassifier model: {f1_score_baseline_dt:.3f}')






#Creating baseline models

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Preprocess the data (assuming df_copy is already preprocessed and ready)
# Split the data into features and target
x = df_copy.drop('Churn', axis=1)
y = df_copy['Churn']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Train a Logistic Regression classifier
lr_clf = LogisticRegression(random_state=42, max_iter=500)
lr_clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = lr_clf.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1_score_baseline_lr = f1_score(y_test, y_pred)
print(f'Accuracy of the Logistic Regression model: {accuracy:.3f}')
print(f'Precision of the Logistic Regression model: {precision:.3f}')
print(f'Recall of the Logistic Regression model: {recall:.3f}')
print(f'F1 Score of the Logistic Regression model: {f1_score_baseline_lr:.3f}')



#Filling up the missing values

#Gender
# Check percentage of missing values in 'gender' column
missing_gender_percent = df['gender'].isnull().sum() / len(df) * 100
print(f"Missing Gender Values: {missing_gender_percent:.2f}%")
# Add a placeholder "Unknown" for missing genders
df['gender'].fillna("Unknown", inplace=True)


#Senior Citizen
#Check for missing values with mode too
missing_senior_citizen_percent = df['SeniorCitizen'].isnull().sum() / len(df) * 100
print(f"Missing SeniorCitizen Values: {missing_senior_citizen_percent:.2f}%")
# Calculate the proportions of the existing values (50% for each class)
senior_dist = df['SeniorCitizen'].value_counts(normalize=True)
# Fill missing values probabilistically based on the existing distribution
df['SeniorCitizen'] = df['SeniorCitizen'].apply(
    lambda senior_lambda: np.random.choice([0.0, 1.0], p=senior_dist.values) if pd.isnull(senior_lambda) else senior_lambda
)


#Partner
missing_partner = df['Partner'].isnull().sum() / len(df) * 100
print(f"Missing Partner Values: {missing_partner:.2f}%")
partner_dist = df['Partner'].value_counts(normalize=True)
df['Partner'] = df['Partner'].apply(lambda partner: np.random.choice(['Yes', 'No'], p=partner_dist.values) if pd.isnull(partner) else partner)

# Dependents
missing_dependents = df['Dependents'].isnull().sum() / len(df) * 100
print(f"Missing Dependents Values: {missing_dependents:.2f}%")
dependent_dist = df['Dependents'].value_counts(normalize=True)
df['Dependents'] = df['Dependents'].apply(lambda dependent: np.random.choice(['Yes', 'No'], p=dependent_dist.values) if pd.isnull(dependent) else dependent)


#Tenure
missing_dependents = df['tenure'].isnull().sum() / len(df) * 100
print(f"Missing tenure Values: {missing_dependents:.2f}%")
df['tenure'].fillna(df['tenure'].median(), inplace=True)  # Fill with median

# PhoneService
missing_phone_service = df['PhoneService'].isnull().sum() / len(df) * 100
print(f"Missing PhoneService Values: {missing_phone_service:.2f}%")
phoneServicer_dist = df['PhoneService'].value_counts(normalize=True)
df['PhoneService'] = df['PhoneService'].apply(lambda phone_servicer: np.random.choice(['Yes', 'No'], p=phoneServicer_dist.values) if pd.isnull(phone_servicer) else phone_servicer)

#MultipleLines
# Calculate the percentage of missing values in the 'MultipleLines' column
missing_multipleLines = df['MultipleLines'].isnull().sum() / len(df) * 100
print(f"Missing MultipleLines Values: {missing_multipleLines:.2f}%")
# Get the distribution of the existing values in the 'MultipleLines' column
multipleLines_dist = df['MultipleLines'].value_counts(normalize=True)

# Fill missing values probabilistically based on the existing distribution
df['MultipleLines'] = df['MultipleLines'].apply(
    lambda x: np.random.choice(multipleLines_dist.index, p=multipleLines_dist.values) if pd.isnull(x) else x
)


# InternetService
missing_internet_service = df['InternetService'].isnull().sum() / len(df) * 100
print(f"Missing InternetService Values: {missing_internet_service:.2f}%")
internet_service_dist = df['InternetService'].value_counts(normalize=True)
df['InternetService'] = df['InternetService'].apply(
    lambda x: np.random.choice(internet_service_dist.index, p=internet_service_dist.values) if pd.isnull(x) else x
)

# OnlineSecurity
missing_online_security = df['OnlineSecurity'].isnull().sum() / len(df) * 100
print(f"Missing OnlineSecurity Values: {missing_online_security:.2f}%")
online_security_dist = df['OnlineSecurity'].value_counts(normalize=True)
df['OnlineSecurity'] = df['OnlineSecurity'].apply(
    lambda x: np.random.choice(online_security_dist.index, p=online_security_dist.values) if pd.isnull(x) else x
)

# OnlineBackup
missing_online_backup = df['OnlineBackup'].isnull().sum() / len(df) * 100
print(f"Missing OnlineBackup Values: {missing_online_backup:.2f}%")
online_backup_dist = df['OnlineBackup'].value_counts(normalize=True)
df['OnlineBackup'] = df['OnlineBackup'].apply(
    lambda x: np.random.choice(online_backup_dist.index, p=online_backup_dist.values) if pd.isnull(x) else x
)



df.isnull().sum()



df.info()


##Encoding the data

# Create a LabelEncoder object for binary features
df.head()
# List of binary columns (for Label Encoding)
binary_cols = ['SeniorCitizen', 'Partner', 'Dependents', 'PhoneService']

# Apply Label Encoding to binary features
le = LabelEncoder()
for col in binary_cols:
    df[col] = le.fit_transform(df[col])

# List of categorical columns (for One-Hot Encoding)
categorical_cols = ['gender', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup']

# Apply One-Hot Encoding
df_preprocessed = pd.get_dummies(df, columns=categorical_cols, drop_first=False, dtype='int')


# Initialize MinMaxScaler
scaler = MinMaxScaler()

# Apply MinMaxScaler to the 'tenure' field and create a new column 'scaled_tenure'
df_preprocessed['scaled_tenure'] = scaler.fit_transform(df[['tenure']])



# Print confirmation
print("DataFrame `df_preprocessed` is ready for model training!")
df_preprocessed.head()


df_preprocessed.describe()



filtered_df = df_preprocessed.drop(columns=['tenure'])
# Box plot for the remaining columns
sns.boxplot(data=filtered_df, orient='h' )

# ax = filtered_df.boxplot(vert=False)

plt.show()


#Finding the best hyperparameters for the Decision tree

x_dt = df_preprocessed.drop(['Churn','scaled_tenure'], axis=1)
y_dt = df_preprocessed['Churn']

# Split data into training and testing sets
x_train_dt, x_test_dt, y_train_dt, y_test_dt = train_test_split(x_dt, y_dt, test_size=0.2, random_state=42)

# Define the refined parameter grid
param_grid = {
    'max_depth': [1, 2, 3, 5, 7],  # Avoiding 'None' since deep trees overfit
    'criterion': ['gini', 'entropy'],
    'min_samples_split': [2, 3, 5],
    'min_samples_leaf': [1, 2, 3]
}

# Initialize GridSearchCV with 5-fold cross-validation
grid_search = GridSearchCV(
    estimator=DecisionTreeClassifier(random_state=42),
    param_grid=param_grid,
    scoring='f1',
    cv=5,
    verbose=1,
    n_jobs=-1
)

# Perform the grid search
grid_search.fit(x_train_dt, y_train_dt)

# Retrieve the best model
best_clf = grid_search.best_estimator_

# Make predictions on the test set using the best model
y_pred_dt = best_clf.predict(x_test_dt)

# Evaluate the best model
accuracy_dt = accuracy_score(y_test_dt, y_pred_dt)
precision_dt = precision_score(y_test_dt, y_pred_dt)
recall_dt = recall_score(y_test_dt, y_pred_dt)
f1_score_dt = f1_score(y_test_dt, y_pred_dt)

# Print the results
print("Best Parameters for Decision Tree Classifier:", grid_search.best_params_)
print(f'Accuracy: {accuracy_dt:.3f}')
print(f'Precision: {precision_dt:.3f}')
print(f'Recall: {recall_dt:.3f}')
print(f'F1 Score: {f1_score_dt:.3f}')



# Decision Tree Classifier
x_dt = df_preprocessed.drop(['Churn','scaled_tenure'], axis=1)
y_dt = df_preprocessed['Churn']

# Define test sizes to evaluate
test_sizes = [0.1, 0.2, 0.3, 0.4]

# Store results for each test size
results_dt = []

for test_size in test_sizes:
    # print(f"\nTesting with test_size = {test_size}")

    # Split the data
    x_train_dt, x_test_dt, y_train_dt, y_test_dt = train_test_split(
        x_dt, y_dt, test_size=test_size, random_state=42
    )

    # Initialize and fit the Decision Tree Classifier with the given hyperparameters
    dt_clf = DecisionTreeClassifier(
        random_state=42,
        criterion='entropy',
        max_depth=7,
        min_samples_leaf=1,
        min_samples_split=2
    )
    dt_clf.fit(x_train_dt, y_train_dt)

    # Make predictions
    y_pred_dt = dt_clf.predict(x_test_dt)

    # Evaluate performance
    accuracy = accuracy_score(y_test_dt, y_pred_dt)
    precision = precision_score(y_test_dt, y_pred_dt, pos_label=1)  # Handle undefined precision
    recall = recall_score(y_test_dt, y_pred_dt, pos_label=1)
    f1 = f1_score(y_test_dt, y_pred_dt, pos_label=1)
    auc_roc_lr = roc_auc_score(y_test_dt, y_pred_dt)


    # Store the results
    results_dt.append((test_size, accuracy, precision, recall, f1, auc_roc_lr ))

    # print(f"Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}")

# Display all results at the end
print("\nSummary of Results of Decision Tree Classifier:")
for i, (test_size, accuracy, precision, recall, f1, auc_roc_lr) in enumerate(results_dt):
    if i == 1:  # Highlight the second record
        print(
            f"\033[1mTest Size: {test_size:.2f} | Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}, AUC-ROC: {auc_roc_lr:.3f}\033[0m")
    else:
        print(
            f"Test Size: {test_size:.2f} | Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}, AUC-ROC: {auc_roc_lr:.3f}")






#Finding the best hyperparameters for the Logistic Regression

# Finding the best hyperparameters for Logistic Regression
x_lr = df_preprocessed.drop(['Churn', 'tenure'], axis=1)
y_lr = df_preprocessed['Churn']

# Split data into training and testing sets
x_train_lr, x_test_lr, y_train_lr, y_test_lr = train_test_split(x_lr, y_lr, test_size=0.2, random_state=42)

# Define the parameter grid for Logistic Regression
# Define the parameter grid
param_grid_lr = [
    {'penalty': ['l1'], 'C': [0.01, 0.1, 1, 10], 'solver': ['liblinear'], 'max_iter': [20, 50, 100, 200]},
    {'penalty': ['l2'], 'C': [0.01, 0.1, 1, 10], 'solver': ['liblinear', 'saga'], 'max_iter': [20, 50, 100, 200]},
    {'penalty': ['elasticnet'], 'C': [0.01, 0.1, 1, 10], 'solver': ['saga'], 'l1_ratio': [0.5], 'max_iter': [20, 50, 100, 200]}
]


# Initialize GridSearchCV
grid_search_lr = GridSearchCV(
    estimator=LogisticRegression(random_state=42),
    param_grid=param_grid_lr,
    scoring='f1',
    cv=5,
    verbose=1,
    n_jobs=-1
)

# Perform the grid search
grid_search_lr.fit(x_train_lr, y_train_lr)

# Retrieve the best model from the search
best_lr_clf = grid_search_lr.best_estimator_

# Make predictions on the test set using the best model
y_pred_lr = best_lr_clf.predict(x_test_lr)

# Evaluate the best model
accuracy_lr = accuracy_score(y_test_lr, y_pred_lr)
precision_lr = precision_score(y_test_lr, y_pred_lr)
recall_lr = recall_score(y_test_lr, y_pred_lr)
f1_score_lr = f1_score(y_test_lr, y_pred_lr)

# Print the results
print("Best Parameters for Logistic Regression Classifier:", grid_search_lr.best_params_)
print(f'Accuracy: {accuracy_lr:.3f}')
print(f'Precision: {precision_lr:.3f}')
print(f'Recall: {recall_lr:.3f}')
print(f'F1 Score: {f1_score_lr:.3f}')

#Capture that we need to filter this via f1 because even though it might lead to some extra marketing spend due to false positives, the higher recall ensures you engage every customer who might leave, which is typically the priority in retention strategies.


#Logistic Regression Classifier
x_lr = df_preprocessed.drop(['Churn','tenure'], axis=1)
y_lr = df_preprocessed['Churn']

# Define possible test sizes
test_sizes = [0.1, 0.2, 0.3, 0.4]

# Store results for each test size
results_lr = []

for test_size in test_sizes:
    # print(f"\nTesting with test_size = {test_size}")

    # Split the data
    x_train_lr, x_test_lr, y_train_lr, y_test_lr = train_test_split(
        x_lr, y_lr, test_size=test_size, random_state=42
    )

    # Initialize and fit the Logistic Regression model
    model = LogisticRegression(
    random_state=42,
    C=0.01,
    l1_ratio=0.5,
    max_iter=200,
    penalty='elasticnet',
    solver='saga',
    )

    model.fit(x_train_lr, y_train_lr)

    # Make predictions
    y_pred = model.predict(x_test_lr)

    # Evaluate performance
    accuracy = accuracy_score(y_test_lr, y_pred)
    precision = precision_score(y_test_lr, y_pred, zero_division=1)  # Handle undefined precision
    recall = recall_score(y_test_lr, y_pred, zero_division=1)
    f1 = f1_score(y_test_lr, y_pred, zero_division=1)
    auc_roc = roc_auc_score(y_test_lr, y_pred)

    # Store the results
    results_lr.append((test_size, accuracy, precision, recall, f1, auc_roc))

    # print(f"Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}")

# Display all results at the end
print("\nSummary of Results of Logistic Regression Classifier:")
for i, (test_size, accuracy, precision, recall, f1, auc_roc) in enumerate(results_lr):
    if i == 1:  # Highlight the second record
        print(
            f"\033[1mTest Size: {test_size:.2f} | Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}, AUC-ROC: {auc_roc:.3f}\033[0m")
    else:
        print(
            f"Test Size: {test_size:.2f} | Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}, AUC-ROC: {auc_roc:.3f}")









# K-Nearest Neighbors Classifier

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Define features and target
x_knn = df_preprocessed.drop(['Churn', 'tenure'], axis=1)
y_knn = df_preprocessed['Churn']

# Define possible test sizes
test_sizes = [0.1, 0.2, 0.3, 0.4]

# Store results for each test size
results_knn = []

for test_size in test_sizes:
    # Split the data
    x_train_knn, x_test_knn, y_train_knn, y_test_knn = train_test_split(
        x_knn, y_knn, test_size=test_size, random_state=42
    )

    # Initialize the KNN model with the specified hyperparameters (Hyper parameters are taken from the grid search)
    knn_model = KNeighborsClassifier(
        metric='euclidean',
        n_neighbors=5,
        weights='distance'
    )

    # Fit the model
    knn_model.fit(x_train_knn, y_train_knn)

    # Make predictions
    y_pred_knn = knn_model.predict(x_test_knn)

    # Evaluate performance
    accuracy = accuracy_score(y_test_knn, y_pred_knn)
    precision = precision_score(y_test_knn, y_pred_knn, zero_division=1)
    recall = recall_score(y_test_knn, y_pred_knn, zero_division=1)
    f1 = f1_score(y_test_knn, y_pred_knn, zero_division=1)
    auc_roc = roc_auc_score(y_test_knn, y_pred_knn)

    # Store the results
    results_knn.append((test_size, accuracy, precision, recall, f1, auc_roc))

# Display all results at the end
print("\nSummary of Results for K-Nearest Neighbors Classifier:")
for i, (test_size, accuracy, precision, recall, f1, auc_roc) in enumerate(results_knn):
    if i == 1:  # Highlight the second record
        print(
            f"\033[1mTest Size: {test_size:.2f} | Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}, AUC-ROC: {auc_roc:.3f}\033[0m")
    else:
        print(
            f"Test Size: {test_size:.2f} | Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}, AUC-ROC: {auc_roc:.3f}")





# Random Forest Classifier

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Feature matrix and target variable
x_rf = df_preprocessed.drop(['Churn', 'tenure'], axis=1)
y_rf = df_preprocessed['Churn']

# Define possible test sizes
test_sizes = [0.1, 0.2, 0.3, 0.4]

# Store results for each test size
results_rf = []

for test_size in test_sizes:
    # Split the data
    x_train_rf, x_test_rf, y_train_rf, y_test_rf = train_test_split(
        x_rf, y_rf, test_size=test_size, random_state=42
    )

    # Initialize Random Forest model with specified hyperparameters
    rf_model = RandomForestClassifier(
        n_estimators=100,  # Number of trees in the forest
        max_depth=None,  # Maximum depth of the tree (None means nodes expand until all leaves are pure)
        random_state=42,  # Random seed for reproducibility
        bootstrap=True,  # Bagging enabled
    )

    # Fit the model on the training data
    rf_model.fit(x_train_rf, y_train_rf)

    # Make predictions on the test data
    y_pred_rf = rf_model.predict(x_test_rf)

    # Evaluate performance
    accuracy = accuracy_score(y_test_rf, y_pred_rf)
    precision = precision_score(y_test_rf, y_pred_rf, zero_division=1)
    recall = recall_score(y_test_rf, y_pred_rf, zero_division=1)
    f1 = f1_score(y_test_rf, y_pred_rf, zero_division=1)
    auc_roc = roc_auc_score(y_test_rf, y_pred_rf)

    # Store results
    results_rf.append((test_size, accuracy, precision, recall, f1, auc_roc))

# Display all results at the end
print("\nSummary of Results for Random Forest Classifier (Bagging):")
for i, (test_size, accuracy, precision, recall, f1, auc_roc) in enumerate(results_rf):
    if i == 1:  # Highlight the second record
        print(
            f"\033[1mTest Size: {test_size:.2f} | Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}, AUC-ROC: {auc_roc:.3f}\033[0m"
        )
    else:
        print(
            f"Test Size: {test_size:.2f} | Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}, AUC-ROC: {auc_roc:.3f}"
        )



import pandas as pd
import plotly.express as px

# Data
models = ['Decision Tree', 'Logistic Regression', 'KNN', 'Random Forest']
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']

# Extract the second (index 1) results for each model and round to 3 decimal places
data = {
    'Decision Tree': [round(metric, 3) for metric in results_dt[1][1:]],  # Skip the test size (1st item)
    'Logistic Regression': [round(metric, 3) for metric in results_lr[1][1:]],  # Skip the test size
    'KNN': [round(metric, 3) for metric in results_knn[1][1:]],  # Skip the test size
    'Random Forest': [round(metric, 3) for metric in results_rf[1][1:]]  # Skip the test size
}

# Print the created data dictionary
# print("Extracted Data (Rounded to 3 Decimal Places):")
# print(data)

# Convert to DataFrame
df_models = pd.DataFrame(data, index=metrics)

# Transform DataFrame into long format for Plotly
df_melted = df_models.reset_index().melt(id_vars='index', var_name='Model', value_name='Score')
df_melted.rename(columns={'index': 'Metric'}, inplace=True)

# Plot using Plotly
fig = px.histogram(
    df_melted,
    x='Metric',  # Metrics on the x-axis
    y='Score',  # Scores on the y-axis
    color='Model',  # Grouped by models
    barmode='group',  # Bars grouped side-by-side
    title='Model Performance Comparison',  # Title of the chart
    color_discrete_sequence=px.colors.qualitative.Prism  # Define color palette
)

# Customize layout
fig.update_layout(
    xaxis_title='Evaluation Metrics',
    yaxis_title='Score',
    width=1000,
    height=500,
    legend_title='Models'
)

# Show the interactive plot
fig.show()




