{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: Arial, sans-serif;\">Customer Churn Prediction in the Telecom Industry: A Machine Learning Approach üìâ</span>\n",
    "\n",
    "Customer churn‚Äîthe discontinuation of a company‚Äôs services‚Äîposes a major challenge in the telecom industry. With **annual churn rates between 15-25%**, reducing customer attrition is a strategic priority, as retaining existing customers is far more cost-effective than acquiring new ones.\n",
    "\n",
    "### **Objectives üéØ**\n",
    "This analysis aims to:\n",
    "\n",
    "* üîç **Explore churn patterns** across customer demographics, service types, and usage behavior.\n",
    "* üìä **Identify key factors contributing to churn** by analyzing correlations and feature importance.\n",
    "* ü§ñ **Build and evaluate predictive models**, including **Logistic Regression, Decision Trees, K-Nearest Neighbors (KNN), and Ensemble Methods**.\n",
    "* üìâ **Compare model performance** using evaluation metrics to determine the most effective approach for churn prediction.\n",
    "\n",
    "By leveraging **machine learning**, this study provides insights into **customer churn trends**, helping telecom companies **identify high-risk customers and improve retention efforts**.\n",
    "\n",
    "üìÇ **Dataset:** `customer_churn_dataset.csv`\n",
    "üìà **Evaluation Metrics:** Precision, Recall, F1-score, and ROC-AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "## REQUIRED LIBRARIES\n",
    "\n",
    "# For data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from plotly.offline import init_notebook_mode\n",
    "\n",
    "# For preprocessing and modeling\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Model building\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üìä Exploratory Data Analysis (EDA)\n",
    "- Understanding the dataset distribution\n",
    "- Checking for missing values and outliers\n",
    "- Identifying feature correlations\n",
    "- Finding key patterns in churn behavior\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = pd.read_csv('customer_churn_dataset.csv')\n",
    "df.head(2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.isnull().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "churned_out_color = '#B71C1C'\n",
    "active_customers_color = '#00BFA5'\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Data Visualization and Exploration \n",
    "# Prepare the data\n",
    "labels = ['Churned Out', 'Active Customers']\n",
    "sizes = [df.Churn[df['Churn'] == 1].count(), df.Churn[df['Churn'] == 0].count()]\n",
    "print(sizes)\n",
    "\n",
    "# Create the pie chart\n",
    "fig = px.pie(\n",
    "    names=labels,\n",
    "    values=sizes,\n",
    "    title=\"Proportion of Customers Churned out and Active Customers\",\n",
    "    hole=0.0,  # For a standard pie chart; set hole=0.5 for a donut chart\n",
    ")\n",
    "\n",
    "# Optional: Tuning visual appearance\n",
    "fig.update_traces(\n",
    "    pull=[0, 0.05],  # Pulls the 'Retained' slice out slightly, similar to \"explode\"\n",
    "    textinfo='percent+label',  # Show percentage and label together\n",
    "    hoverinfo='label+percent+value',  # Hover information\n",
    "    marker=dict(line=dict(color='black', width=0.5),colors=[churned_out_color, active_customers_color]),  # Customize marker line\n",
    ")\n",
    "\n",
    "# Adjust the layout to set the width and height\n",
    "fig.update_layout(\n",
    "    width=800,  # Set desired width (e.g., 600 pixels)\n",
    "    height=500  # Set desired height (e.g., 400 pixels)\n",
    ")\n",
    "\n",
    "\n",
    "# Show the chart\n",
    "fig.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Prepare data for analysis and exploration\n",
    "# - Create a copy of the original DataFrame for exploratory data analysis (EDA)\n",
    "# - Remove the 'customerID' column as it is irrelevant for modeling\n",
    "# - Map categorical values in 'Churn' and 'SeniorCitizen' columns to more meaningful labels\n",
    "#   for better readability and interpretation\n",
    "\n",
    "df_copy = df.copy()\n",
    "\n",
    "# Drop the customerID column\n",
    "if 'customerID' in df.columns:\n",
    "    df = df.drop(columns=['customerID'])\n",
    "\n",
    "# Drop the customerID column\n",
    "if 'customerID' in df_copy.columns:\n",
    "    df_copy = df_copy.drop(columns=['customerID'])\n",
    "\n",
    "# Map the Churn column to the desired labels in the copy\n",
    "df_copy['Churn'] = df_copy['Churn'].map({0: 'Active Customers', 1: 'Churned Out'})\n",
    "df_copy['SeniorCitizen'] = df_copy['SeniorCitizen'].map({0: 'Non-Senior Citizen', 1: 'Senior Citizen'})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Gender\n",
    "fig = px.histogram(df_copy,\n",
    "                   x='gender',\n",
    "                   color='Churn',\n",
    "                   title='Churn Rate by Gender',\n",
    "                   barmode='group',\n",
    "                   color_discrete_sequence=[churned_out_color,active_customers_color],)\n",
    "\n",
    "fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#SeniorCitizen\n",
    "fig = px.histogram(df_copy,\n",
    "                   x='SeniorCitizen',\n",
    "                   color='Churn',\n",
    "                   title='Churn Rate by Senior Citizen',\n",
    "                   barmode='group',\n",
    "                   color_discrete_sequence=[churned_out_color,active_customers_color],)\n",
    "\n",
    "fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)\n",
    "fig.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Partner\n",
    "fig = px.histogram(df_copy,\n",
    "                   x='Partner',\n",
    "                   color='Churn',\n",
    "                   title='Churn Rate by Partner',\n",
    "                   barmode='group',\n",
    "                   color_discrete_sequence=[churned_out_color,active_customers_color],)\n",
    "\n",
    "fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Dependents\n",
    "fig = px.histogram(df_copy,\n",
    "                   x='Dependents',\n",
    "                   color='Churn',\n",
    "                   title='Churn Rate by Dependents',\n",
    "                   barmode='group',\n",
    "                   color_discrete_sequence=[churned_out_color,active_customers_color],)\n",
    "\n",
    "fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#PhoneService\n",
    "fig = px.histogram(df_copy,\n",
    "                   x='PhoneService',\n",
    "                   color='Churn',\n",
    "                   title='Churn Rate by PhoneService',\n",
    "                   barmode='group',\n",
    "                   color_discrete_sequence=[churned_out_color,active_customers_color],)\n",
    "\n",
    "fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#MultipleLines\n",
    "fig = px.histogram(df_copy,\n",
    "                   x='MultipleLines',\n",
    "                   color='Churn',\n",
    "                   title='Churn Rate by MultipleLines',\n",
    "                   barmode='group',\n",
    "                   color_discrete_sequence=[churned_out_color,active_customers_color],)\n",
    "\n",
    "fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#InternetService\n",
    "fig = px.histogram(df_copy,\n",
    "                   x='InternetService',\n",
    "                   color='Churn',\n",
    "                   title='Churn Rate by InternetService',\n",
    "                   barmode='group',\n",
    "                   color_discrete_sequence=[churned_out_color,active_customers_color],)\n",
    "\n",
    "fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#OnlineSecurity\n",
    "fig = px.histogram(df_copy,\n",
    "                   x='OnlineSecurity',\n",
    "                   color='Churn',\n",
    "                   title='Churn Rate by OnlineSecurity',\n",
    "                   barmode='group',\n",
    "                   color_discrete_sequence=[churned_out_color,active_customers_color],)\n",
    "\n",
    "fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#OnlineBackup\n",
    "fig = px.histogram(df_copy,\n",
    "                   x='OnlineBackup',\n",
    "                   color='Churn',\n",
    "                   title='Churn Rate by OnlineBackup',\n",
    "                   barmode='group',\n",
    "                   color_discrete_sequence=[churned_out_color,active_customers_color],)\n",
    "\n",
    "fig.update_layout(xaxis_title='Active Customers vs Churned out', yaxis_title='Count', width=800, height=400)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#tenure\n",
    "# Group and aggregate data\n",
    "grouped_data = df_copy.groupby(['tenure', 'Churn']).size().reset_index(name='Customer Count')\n",
    "\n",
    "# Create the line chart\n",
    "fig = px.line(\n",
    "    grouped_data,\n",
    "    x='tenure',\n",
    "    y='Customer Count',\n",
    "    color='Churn',\n",
    "    title='Churn Rate by Tenure',\n",
    "    color_discrete_sequence=[active_customers_color,churned_out_color]\n",
    ")\n",
    "\n",
    "# Update layout for better labels\n",
    "fig.update_layout(\n",
    "    xaxis_title='Tenure',\n",
    "    yaxis_title='Customer Count',\n",
    "    legend_title='Churn Status',\n",
    "\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Key Observations from Customer Churn Analysis\n",
    "\n",
    "We note the following insights from the visualizations:\n",
    "\n",
    "üìå **Churn Rate is Nearly 50%**\n",
    "- The dataset contains **5,020 churned customers** and **4,980 non-churned customers**, making churn prediction an important task.\n",
    "\n",
    "üìà **Most Features Show Similar Distributions**\n",
    "- **Gender, Partner, Dependents, PhoneService, MultipleLines, OnlineSecurity, and OnlineBackup** all have **nearly equal proportions** between churned and non-churned customers.\n",
    "- This suggests that **these individual features alone are not strong predictors of churn**.\n",
    "\n",
    "üìÑ **Tenure Shows a Clear Pattern**\n",
    "- Customers with **shorter tenure (0-20 months)** exhibit **higher churn rates**, indicating that early-stage customers are more likely to leave.\n",
    "- Churn **fluctuates but stabilizes** beyond **30 months**, though there are intermittent spikes.\n",
    "- Understanding **contract renewals, pricing changes, or service issues** at these peaks can provide deeper insights.\n",
    "- **Retention strategies should focus on early-tenure customers**, potentially through personalized offers or improved onboarding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Drop rows with missing values\n",
    "df_copy = df_copy.dropna()\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for column in df_copy.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    df_copy[column] = le.fit_transform(df_copy[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = df_copy.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Understanding the Correlation Matrix\n",
    "\n",
    "The **correlation matrix** shows how features relate to each other and to churn. Key takeaways:\n",
    "\n",
    "üìå **No Strong Correlation with Churn**\n",
    "- All features have **low correlation values** with churn, meaning **no single feature alone is a strong predictor**.\n",
    "- **Tenure shows a slight negative correlation**, indicating that customers with longer tenure are less likely to churn.\n",
    "\n",
    "üìå **Minimal Multicollinearity**\n",
    "- No two features are highly correlated, meaning **redundant features are unlikely**.\n",
    "- This suggests **feature interactions** might be more important than individual features.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Why Analyze Feature Importance?\n",
    "\n",
    "Since correlation alone doesn't tell us **how much each feature contributes to churn**, we need to evaluate **feature importance**:\n",
    "\n",
    "‚úÖ **Identify which features have the most impact** on predictions.\n",
    "‚úÖ **Go beyond simple correlations** by capturing non-linear relationships.\n",
    "‚úÖ **Prioritize key factors** to improve churn modeling and business strategies.\n",
    "\n",
    "To achieve this, we use **RandomForestClassifier**, which ranks features based on their contribution to decision-making. This helps confirm whether **features like tenure and contract type are indeed the strongest predictors**.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# Preprocess the data\n",
    "df_copy = df_copy.dropna()  # Drop rows with missing values\n",
    "label_encoders = {}\n",
    "for column in df_copy.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    df_copy[column] = le.fit_transform(df_copy[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df_copy.drop('Churn', axis=1)\n",
    "y = df_copy['Churn']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a random forest classifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "# Print feature importance\n",
    "print(feature_importance)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T04:59:21.216254Z",
     "start_time": "2025-02-27T04:59:21.211180Z"
    }
   },
   "source": [
    "## ‚ö° Why Create Baseline Models?\n",
    "\n",
    "Before building a complex model, it's essential to **establish baseline performance** using simpler models. This helps in:\n",
    "\n",
    "‚úÖ **Setting a reference point** ‚Äì Helps measure improvement when testing more advanced models.\n",
    "‚úÖ **Identifying initial patterns** ‚Äì Even simple models can highlight key predictive features.\n",
    "‚úÖ **Balancing interpretability and performance** ‚Äì Decision Trees and Logistic Regression provide insight into feature importance and separability.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Baseline Models: Decision Tree & Logistic Regression\n",
    "\n",
    "To create a solid starting point, we train **two different models**:\n",
    "\n",
    "1Ô∏è‚É£ **Decision Tree Classifier**\n",
    "- Captures **non-linear relationships** and **feature interactions**.\n",
    "- Helps identify **key decision-making splits** for churn prediction.\n",
    "\n",
    "2Ô∏è‚É£ **Logistic Regression**\n",
    "- A **simple, interpretable model** that provides **probabilities of churn**.\n",
    "- Acts as a benchmark to compare against more complex models.\n",
    "\n",
    "### üîé Key Metrics Evaluated\n",
    "We evaluate both models using:\n",
    "- **Accuracy** ‚Äì Overall correctness.\n",
    "- **Precision** ‚Äì How many predicted churns were correct.\n",
    "- **Recall** ‚Äì How many actual churn cases were detected.\n",
    "- **F1 Score** ‚Äì A balance of precision and recall.\n",
    "\n",
    "These baselines allow us to **compare future models** and ensure that advanced techniques actually provide **real improvements** over simpler methods. üöÄ\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Creating baseline models\n",
    "# Preprocess the data (assuming df_copy is already preprocessed and ready)\n",
    "# Split the data into features and target\n",
    "x = df_copy.drop('Churn', axis=1)\n",
    "y = df_copy['Churn']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Decision Tree classifier\n",
    "dt_clf = DecisionTreeClassifier(random_state=42, criterion='entropy', max_depth=5, )\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = dt_clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1_score_baseline_dt = f1_score(y_test, y_pred)\n",
    "print(f'Accuracy of the DecisionTreeClassifier model: {accuracy:.3f}')\n",
    "print(f'Precision of the DecisionTreeClassifier model: {precision:.3f}')\n",
    "print(f'Recall of the DecisionTreeClassifier model: {recall:.3f}')\n",
    "print(f'F1 Score of the DecisionTreeClassifier model: {f1_score_baseline_dt:.3f}')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Creating baseline models\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Preprocess the data (assuming df_copy is already preprocessed and ready)\n",
    "# Split the data into features and target\n",
    "x = df_copy.drop('Churn', axis=1)\n",
    "y = df_copy['Churn']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression classifier\n",
    "lr_clf = LogisticRegression(random_state=42, max_iter=500)\n",
    "lr_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = lr_clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1_score_baseline_lr = f1_score(y_test, y_pred)\n",
    "print(f'Accuracy of the Logistic Regression model: {accuracy:.3f}')\n",
    "print(f'Precision of the Logistic Regression model: {precision:.3f}')\n",
    "print(f'Recall of the Logistic Regression model: {recall:.3f}')\n",
    "print(f'F1 Score of the Logistic Regression model: {f1_score_baseline_lr:.3f}')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# üõ†Ô∏è Data Cleaning & Preprocessing\n",
    "- Handling missing values\n",
    "- Encoding categorical variables\n",
    "- Feature selection and scaling"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Filling up the missing values\n",
    "\n",
    "#Gender\n",
    "missing_gender_percent = df['gender'].isnull().sum() / len(df) * 100\n",
    "print(f\"Missing Gender Values: {missing_gender_percent:.2f}%\")\n",
    "df.loc[df['gender'].isnull(), 'gender'] = \"Unknown\"\n",
    "\n",
    "#Senior Citizen\n",
    "missing_senior_citizen_percent = df['SeniorCitizen'].isnull().sum() / len(df) * 100\n",
    "print(f\"Missing SeniorCitizen Values: {missing_senior_citizen_percent:.2f}%\")\n",
    "senior_dist = df['SeniorCitizen'].value_counts(normalize=True)\n",
    "df.loc[df['SeniorCitizen'].isnull(), 'SeniorCitizen'] = np.random.choice([0.0, 1.0], p=senior_dist.values)\n",
    "\n",
    "#Partner\n",
    "missing_partner = df['Partner'].isnull().sum() / len(df) * 100\n",
    "print(f\"Missing Partner Values: {missing_partner:.2f}%\")\n",
    "partner_dist = df['Partner'].value_counts(normalize=True)\n",
    "df.loc[df['Partner'].isnull(), 'Partner'] = np.random.choice(['Yes', 'No'], p=partner_dist.values)\n",
    "\n",
    "#Dependents\n",
    "missing_dependents = df['Dependents'].isnull().sum() / len(df) * 100\n",
    "print(f\"Missing Dependents Values: {missing_dependents:.2f}%\")\n",
    "dependent_dist = df['Dependents'].value_counts(normalize=True)\n",
    "df.loc[df['Dependents'].isnull(), 'Dependents'] = np.random.choice(['Yes', 'No'], p=dependent_dist.values)\n",
    "\n",
    "#Tenure\n",
    "missing_tenure = df['tenure'].isnull().sum() / len(df) * 100\n",
    "print(f\"Missing Tenure Values: {missing_tenure:.2f}%\")\n",
    "df.loc[df['tenure'].isnull(), 'tenure'] = df['tenure'].median()\n",
    "\n",
    "#Phone Service\n",
    "missing_phone_service = df['PhoneService'].isnull().sum() / len(df) * 100\n",
    "print(f\"Missing PhoneService Values: {missing_phone_service:.2f}%\")\n",
    "phone_service_dist = df['PhoneService'].value_counts(normalize=True)\n",
    "df.loc[df['PhoneService'].isnull(), 'PhoneService'] = np.random.choice(['Yes', 'No'], p=phone_service_dist.values)\n",
    "\n",
    "#Multiple Lines\n",
    "missing_multiple_lines = df['MultipleLines'].isnull().sum() / len(df) * 100\n",
    "print(f\"Missing MultipleLines Values: {missing_multiple_lines:.2f}%\")\n",
    "multiple_lines_dist = df['MultipleLines'].value_counts(normalize=True)\n",
    "df.loc[df['MultipleLines'].isnull(), 'MultipleLines'] = np.random.choice(multiple_lines_dist.index, p=multiple_lines_dist.values)\n",
    "\n",
    "#Internet Service\n",
    "missing_internet_service = df['InternetService'].isnull().sum() / len(df) * 100\n",
    "print(f\"Missing InternetService Values: {missing_internet_service:.2f}%\")\n",
    "internet_service_dist = df['InternetService'].value_counts(normalize=True)\n",
    "df.loc[df['InternetService'].isnull(), 'InternetService'] = np.random.choice(internet_service_dist.index, p=internet_service_dist.values)\n",
    "\n",
    "#Online Security\n",
    "missing_online_security = df['OnlineSecurity'].isnull().sum() / len(df) * 100\n",
    "print(f\"Missing OnlineSecurity Values: {missing_online_security:.2f}%\")\n",
    "online_security_dist = df['OnlineSecurity'].value_counts(normalize=True)\n",
    "df.loc[df['OnlineSecurity'].isnull(), 'OnlineSecurity'] = np.random.choice(online_security_dist.index, p=online_security_dist.values)\n",
    "\n",
    "#Online Backup\n",
    "missing_online_backup = df['OnlineBackup'].isnull().sum() / len(df) * 100\n",
    "print(f\"Missing OnlineBackup Values: {missing_online_backup:.2f}%\")\n",
    "online_backup_dist = df['OnlineBackup'].value_counts(normalize=True)\n",
    "df.loc[df['OnlineBackup'].isnull(), 'OnlineBackup'] = np.random.choice(online_backup_dist.index, p=online_backup_dist.values)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ† Handling Missing Values\n",
    "\n",
    "### üßë‚Äçü§ù‚Äçüßë Gender\n",
    "- **Missing values replaced with `\"Unknown\"`** instead of imputing a category.\n",
    "- ‚úÖ **Why?** Since gender is categorical and missing values are not predictable, it's better to keep them explicit rather than introducing bias.\n",
    "\n",
    "### üë¥ Senior Citizen\n",
    "- **Filled probabilistically** based on the distribution of existing values.\n",
    "- ‚úÖ **Why?** Maintains the **real-world proportion** instead of defaulting to a specific class.\n",
    "\n",
    "### üíë Partner & üçº Dependents\n",
    "- **Filled probabilistically** based on the existing ratio of \"Yes\"/\"No\".\n",
    "- ‚úÖ **Why?** Prevents over-representing either category and ensures realistic data patterns.\n",
    "\n",
    "### üìä Tenure\n",
    "- **Filled with the median** instead of the mean.\n",
    "- ‚úÖ **Why?** The median is **less sensitive to outliers**, ensuring a more balanced distribution.\n",
    "\n",
    "### üìû Phone Service & üì∂ Multiple Lines\n",
    "- **Filled probabilistically** using the distribution of available values.\n",
    "- ‚úÖ **Why?** Helps maintain the service adoption rate in the dataset.\n",
    "\n",
    "### üåê Internet Service\n",
    "- **Filled probabilistically** using the existing category proportions.\n",
    "- ‚úÖ **Why?** Ensures that the distribution of different service types remains realistic.\n",
    "\n",
    "### üîê Online Security & üìÅ Online Backup\n",
    "- **Filled probabilistically** based on category frequencies.\n",
    "- ‚úÖ **Why?** Retains natural variations rather than over-sampling any single category.\n",
    "\n",
    "### üîπ **Why is probabilistic filling better?**\n",
    "- **Prevents bias** ‚Äì avoids over-representing any one category.\n",
    "- **Mimics real-world patterns** ‚Äì missing data is distributed naturally.\n",
    "- **More accurate predictions** ‚Äì models learn from a dataset that reflects actual trends.\n",
    "\n",
    "üöÄ **Now, our dataset is clean, consistent, and ready for analysis!**\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.isnull().sum()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "##Encoding the data\n",
    "\n",
    "# Create a LabelEncoder object for binary features\n",
    "df.head()\n",
    "# List of binary columns (for Label Encoding)\n",
    "binary_cols = ['SeniorCitizen', 'Partner', 'Dependents', 'PhoneService']\n",
    "\n",
    "# Apply Label Encoding to binary features\n",
    "le = LabelEncoder()\n",
    "for col in binary_cols:\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# List of categorical columns (for One-Hot Encoding)\n",
    "categorical_cols = ['gender', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup']\n",
    "\n",
    "# Apply One-Hot Encoding\n",
    "df_preprocessed = pd.get_dummies(df, columns=categorical_cols, drop_first=False, dtype='int')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply MinMaxScaler to the 'tenure' field and create a new column 'scaled_tenure'\n",
    "df_preprocessed['scaled_tenure'] = scaler.fit_transform(df[['tenure']])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî† Encoding the Data\n",
    "\n",
    "To prepare the dataset for machine learning, we need to **convert categorical variables into numerical form**.\n",
    "\n",
    "### üèÅÔ∏è Label Encoding (Binary Features)\n",
    "- Applied to **binary columns**: `SeniorCitizen`, `Partner`, `Dependents`, `PhoneService`.\n",
    "- ‚úÖ **Why?** These features have only **two categories** (`Yes/No` or `0/1`), making them suitable for **Label Encoding**.\n",
    "\n",
    "### üß© One-Hot Encoding (Categorical Features)\n",
    "- Applied to **multi-category columns**: `gender`, `MultipleLines`, `InternetService`, `OnlineSecurity`, `OnlineBackup`.\n",
    "- ‚úÖ **Why?** One-hot encoding **creates separate columns** for each category, allowing models to handle non-ordinal data correctly.\n",
    "\n",
    "---\n",
    "\n",
    "## üìè Scaling the Data\n",
    "\n",
    "### üìè MinMax Scaling (`tenure`)\n",
    "- **Scaled tenure** using `MinMaxScaler` to **normalize values between 0 and 1**.\n",
    "- ‚úÖ **Why?** Ensures that tenure **does not dominate other features** due to its larger range.\n",
    "\n",
    "üöÄ **Now, our dataset is fully encoded, scaled, and ready for model training!**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Print confirmation\n",
    "print(\"DataFrame `df_preprocessed` is ready for model training!\")\n",
    "df_preprocessed.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_preprocessed.describe()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Drop the 'tenure' column\n",
    "filtered_df = df_preprocessed.drop(columns=['tenure'])\n",
    "\n",
    "# Convert DataFrame to long format for Plotly\n",
    "df_melted = filtered_df.melt(var_name='Feature', value_name='Value')\n",
    "\n",
    "# Create an interactive box plot with thicker elements\n",
    "fig = px.box(\n",
    "    df_melted, \n",
    "    x='Value', \n",
    "    y='Feature', \n",
    "    title=\"Box Plot of Features\",\n",
    "    color='Feature',  # Different colors for each feature\n",
    "    color_discrete_sequence=px.colors.qualitative.Prism  # Color palette\n",
    ")\n",
    "\n",
    "# Increase thickness of box elements\n",
    "fig.update_traces(\n",
    "    boxmean=True,  # Show mean as a line inside the box\n",
    "    marker=dict(size=6),  # Make outlier points bigger\n",
    "    line=dict(width=3)  # Make box plot lines thicker\n",
    ")\n",
    "\n",
    "# Improve layout\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Value Distribution\",\n",
    "    yaxis_title=\"Features\",\n",
    "    width=900,\n",
    "    height=500,\n",
    "    font=dict(family=\"Arial, sans-serif\", size=12, color=\"black\"),\n",
    "    margin=dict(l=100, r=50, t=50, b=50)  # Adjust margins\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ü§ñ Machine Learning Models\n",
    "- Creating models (Decision Tree, Logistic Regression, KNN, RandomForest Classifier)\n",
    "- Evaluating performance (Accuracy, Precision, Recall, F1-score)\n",
    "- Identifying important features for churn prediction\n",
    "- Improving model performance with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Checking model building with manual tuning of hyperparameters - Decision Tree\n",
    "\n",
    "# Decision Tree Classifier - Test Size = 0.2\n",
    "x_dt = df_preprocessed.drop(['Churn', 'scaled_tenure'], axis=1)\n",
    "y_dt = df_preprocessed['Churn']\n",
    "\n",
    "# Split the data with test_size = 0.2\n",
    "x_train_dt, x_test_dt, y_train_dt, y_test_dt = train_test_split(\n",
    "    x_dt, y_dt, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize and fit the Decision Tree Classifier with the given hyperparameters (manual tuning)\n",
    "dt_clf = DecisionTreeClassifier(\n",
    "    random_state=42,\n",
    "    criterion='entropy',\n",
    "    max_depth=7,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2\n",
    ")\n",
    "dt_clf.fit(x_train_dt, y_train_dt)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_dt = dt_clf.predict(x_test_dt)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test_dt, y_pred_dt)\n",
    "precision = precision_score(y_test_dt, y_pred_dt, pos_label=1)\n",
    "recall = recall_score(y_test_dt, y_pred_dt, pos_label=1)\n",
    "f1 = f1_score(y_test_dt, y_pred_dt, pos_label=1)\n",
    "auc_roc = roc_auc_score(y_test_dt, y_pred_dt)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nResults of Decision Tree Classifier with Test Size = 0.2:\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-Score: {f1:.3f}\")\n",
    "print(f\"AUC-ROC: {auc_roc:.3f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Finding the best hyperparameters for the Decision tree with Grid Search CV\n",
    "\n",
    "x_dt = df_preprocessed.drop(['Churn','scaled_tenure'], axis=1)\n",
    "y_dt = df_preprocessed['Churn']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "x_train_dt, x_test_dt, y_train_dt, y_test_dt = train_test_split(x_dt, y_dt, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the refined parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3, 5, 7],  # Avoiding 'None' since deep trees overfit\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': [2, 3, 5],\n",
    "    'min_samples_leaf': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with 5-fold cross-validation\n",
    "grid_search_dt = GridSearchCV(\n",
    "    estimator=DecisionTreeClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search_dt.fit(x_train_dt, y_train_dt)\n",
    "\n",
    "# Retrieve the best model\n",
    "best_clf = grid_search_dt.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_dt = best_clf.predict(x_test_dt)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_dt = accuracy_score(y_test_dt, y_pred_dt)\n",
    "precision_dt = precision_score(y_test_dt, y_pred_dt)\n",
    "recall_dt = recall_score(y_test_dt, y_pred_dt)\n",
    "f1_score_dt = f1_score(y_test_dt, y_pred_dt)\n",
    "\n",
    "# Print the results\n",
    "print(\"Best Parameters for Decision Tree Classifier:\", grid_search_dt.best_params_)\n",
    "print(f'Accuracy: {accuracy_dt:.3f}')\n",
    "print(f'Precision: {precision_dt:.3f}')\n",
    "print(f'Recall: {recall_dt:.3f}')\n",
    "print(f'F1 Score: {f1_score_dt:.3f}')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier\n",
    "x_dt = df_preprocessed.drop(['Churn','scaled_tenure'], axis=1)\n",
    "y_dt = df_preprocessed['Churn']\n",
    "\n",
    "# Define test sizes to evaluate\n",
    "test_sizes = [0.1, 0.2, 0.3, 0.4]\n",
    "\n",
    "# Store results for each test size\n",
    "results_dt = []\n",
    "\n",
    "for test_size in test_sizes:\n",
    "    # print(f\"\\nTesting with test_size = {test_size}\")\n",
    "\n",
    "    # Split the data\n",
    "    x_train_dt, x_test_dt, y_train_dt, y_test_dt = train_test_split(\n",
    "        x_dt, y_dt, test_size=test_size, random_state=42\n",
    "    )\n",
    "\n",
    "    # Initialize and fit the Decision Tree Classifier with the given hyperparameters\n",
    "    dt_clf = DecisionTreeClassifier(**grid_search_dt.best_params_)\n",
    "    dt_clf.fit(x_train_dt, y_train_dt)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred_dt = dt_clf.predict(x_test_dt)\n",
    "\n",
    "    # Evaluate performance\n",
    "    accuracy = accuracy_score(y_test_dt, y_pred_dt)\n",
    "    precision = precision_score(y_test_dt, y_pred_dt, pos_label=1)  # Handle undefined precision\n",
    "    recall = recall_score(y_test_dt, y_pred_dt, pos_label=1)\n",
    "    f1 = f1_score(y_test_dt, y_pred_dt, pos_label=1)\n",
    "    auc_roc_lr = roc_auc_score(y_test_dt, y_pred_dt)\n",
    "\n",
    "\n",
    "    # Store the results\n",
    "    results_dt.append((test_size, accuracy, precision, recall, f1, auc_roc_lr ))\n",
    "\n",
    "    # print(f\"Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}\")\n",
    "\n",
    "# Display all results at the end\n",
    "print(\"\\nSummary of Results of Decision Tree Classifier:\")\n",
    "for i, (test_size, accuracy, precision, recall, f1, auc_roc_lr) in enumerate(results_dt):\n",
    "    if i == 1:  # Highlight the second record\n",
    "        print(\n",
    "            f\"\\033[1mTest Size: {test_size:.2f} | Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}, AUC-ROC: {auc_roc_lr:.3f}\\033[0m\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"Test Size: {test_size:.2f} | Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}, AUC-ROC: {auc_roc_lr:.3f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üìå Decision Tree Classifier - Hyperparameter Tuning & Evaluation\n",
    "\n",
    "## 1Ô∏è‚É£ Manual Hyperparameter Tuning\n",
    "- A **Decision Tree Classifier** is trained with manually set hyperparameters.\n",
    "- The model is evaluated using **Accuracy, Precision, Recall, F1-Score, and AUC-ROC** to measure performance.\n",
    "\n",
    "## 2Ô∏è‚É£ Finding Best Hyperparameters with GridSearchCV\n",
    "- **GridSearchCV** is used to identify the best combination of hyperparameters.\n",
    "- The search is performed over different values of **`max_depth`**, **`criterion`**, **`min_samples_split`**, and **`min_samples_leaf`**.\n",
    "- The model is evaluated using **5-fold cross-validation** with **F1-score** as the scoring metric.\n",
    "\n",
    "## 3Ô∏è‚É£ Evaluating the Best Model on Different Test Splits\n",
    "- The best parameters from **GridSearchCV** are used to train and test models across different **test sizes (0.1, 0.2, 0.3, 0.4)**.\n",
    "- The performance of each model is compared using **Accuracy, Precision, Recall, F1-Score, and AUC-ROC** to analyze the impact of different test splits.\n",
    "\n",
    "üîπ **This process ensures that the model is well-optimized and generalizes effectively across different data splits.**\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Manual tuning of hyperparameters for Decision Logistic Regression\n",
    "\n",
    "# Logistic Regression Classifier - Test Size = 0.2\n",
    "x_lr = df_preprocessed.drop(['Churn', 'tenure'], axis=1)\n",
    "y_lr = df_preprocessed['Churn']\n",
    "\n",
    "# Split the data with test_size = 0.2\n",
    "x_train_lr, x_test_lr, y_train_lr, y_test_lr = train_test_split(\n",
    "    x_lr, y_lr, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize and fit the Logistic Regression model\n",
    "model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    C=0.01,\n",
    "    l1_ratio=0.5,\n",
    "    max_iter=200,\n",
    "    penalty='elasticnet',\n",
    "    solver='saga'\n",
    ")\n",
    "\n",
    "model.fit(x_train_lr, y_train_lr)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(x_test_lr)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test_lr, y_pred)\n",
    "precision = precision_score(y_test_lr, y_pred, zero_division=1)  # Handle undefined precision\n",
    "recall = recall_score(y_test_lr, y_pred, zero_division=1)\n",
    "f1 = f1_score(y_test_lr, y_pred, zero_division=1)\n",
    "auc_roc = roc_auc_score(y_test_lr, y_pred)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nResults of Logistic Regression Classifier with Test Size = 0.2 & Manual tuning the hyperparameters\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-Score: {f1:.3f}\")\n",
    "print(f\"AUC-ROC: {auc_roc:.3f}\")\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Finding the best hyperparameters for the Logistic Regression\n",
    "\n",
    "# Finding the best hyperparameters for Logistic Regression\n",
    "x_lr = df_preprocessed.drop(['Churn', 'tenure'], axis=1)\n",
    "y_lr = df_preprocessed['Churn']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "x_train_lr, x_test_lr, y_train_lr, y_test_lr = train_test_split(x_lr, y_lr, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for Logistic Regression\n",
    "# Define the parameter grid\n",
    "param_grid_lr = [\n",
    "    {'penalty': ['l1'], 'C': [0.01, 0.1, 1, 10], 'solver': ['liblinear'], 'max_iter': [20, 50, 100, 200]},\n",
    "    {'penalty': ['l2'], 'C': [0.01, 0.1, 1, 10], 'solver': ['liblinear', 'saga'], 'max_iter': [20, 50, 100, 200]},\n",
    "    {'penalty': ['elasticnet'], 'C': [0.01, 0.1, 1, 10], 'solver': ['saga'], 'l1_ratio': [0.5], 'max_iter': [20, 50, 100, 200]}\n",
    "]\n",
    "\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_lr = GridSearchCV(\n",
    "    estimator=LogisticRegression(random_state=42),\n",
    "    param_grid=param_grid_lr,\n",
    "    scoring='f1',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search_lr.fit(x_train_lr, y_train_lr)\n",
    "\n",
    "# Retrieve the best model from the search\n",
    "best_lr_clf = grid_search_lr.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_lr = best_lr_clf.predict(x_test_lr)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_lr = accuracy_score(y_test_lr, y_pred_lr)\n",
    "precision_lr = precision_score(y_test_lr, y_pred_lr)\n",
    "recall_lr = recall_score(y_test_lr, y_pred_lr)\n",
    "f1_score_lr = f1_score(y_test_lr, y_pred_lr)\n",
    "\n",
    "# Print the results\n",
    "print(\"Best Parameters for Logistic Regression Classifier:\", grid_search_lr.best_params_)\n",
    "print(f'Accuracy: {accuracy_lr:.3f}')\n",
    "print(f'Precision: {precision_lr:.3f}')\n",
    "print(f'Recall: {recall_lr:.3f}')\n",
    "print(f'F1 Score: {f1_score_lr:.3f}')\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Logistic Regression Classifier\n",
    "x_lr = df_preprocessed.drop(['Churn','tenure'], axis=1)\n",
    "y_lr = df_preprocessed['Churn']\n",
    "\n",
    "# Define possible test sizes\n",
    "test_sizes = [0.1, 0.2, 0.3, 0.4]\n",
    "\n",
    "# Store results for each test size\n",
    "results_lr = []\n",
    "\n",
    "for test_size in test_sizes:\n",
    "    # print(f\"\\nTesting with test_size = {test_size}\")\n",
    "\n",
    "    # Split the data\n",
    "    x_train_lr, x_test_lr, y_train_lr, y_test_lr = train_test_split(\n",
    "        x_lr, y_lr, test_size=test_size, random_state=42\n",
    "    )\n",
    "\n",
    "    # Initialize and fit the Logistic Regression model\n",
    "    model = LogisticRegression(**grid_search_lr.best_params_)\n",
    "\n",
    "    model.fit(x_train_lr, y_train_lr)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(x_test_lr)\n",
    "\n",
    "    # Evaluate performance\n",
    "    accuracy = accuracy_score(y_test_lr, y_pred)\n",
    "    precision = precision_score(y_test_lr, y_pred, zero_division=1)  # Handle undefined precision\n",
    "    recall = recall_score(y_test_lr, y_pred, zero_division=1)\n",
    "    f1 = f1_score(y_test_lr, y_pred, zero_division=1)\n",
    "    auc_roc = roc_auc_score(y_test_lr, y_pred)\n",
    "\n",
    "    # Store the results\n",
    "    results_lr.append((test_size, accuracy, precision, recall, f1, auc_roc))\n",
    "\n",
    "    # print(f\"Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}\")\n",
    "\n",
    "# Display all results at the end\n",
    "print(\"\\nSummary of Results of Logistic Regression Classifier:\")\n",
    "for i, (test_size, accuracy, precision, recall, f1, auc_roc) in enumerate(results_lr):\n",
    "    if i == 1:  # Highlight the second record\n",
    "        print(\n",
    "            f\"\\033[1mTest Size: {test_size:.2f} | Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}, AUC-ROC: {auc_roc:.3f}\\033[0m\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"Test Size: {test_size:.2f} | Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}, AUC-ROC: {auc_roc:.3f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üìå Logistic Regression Classifier - Hyperparameter Tuning & Evaluation\n",
    "\n",
    "## 1Ô∏è‚É£ Manual Hyperparameter Tuning\n",
    "- A **Logistic Regression model** is trained with manually set hyperparameters.\n",
    "- The model is evaluated using **Accuracy, Precision, Recall, F1-Score, and AUC-ROC** to measure performance.\n",
    "\n",
    "## 2Ô∏è‚É£ Finding Best Hyperparameters with GridSearchCV\n",
    "- **GridSearchCV** is used to optimize the **`penalty`**, **`C`**, **`solver`**, and **`max_iter`** values.\n",
    "- The search is performed using **5-fold cross-validation** with **F1-score as the metric**.\n",
    "- **Why F1-score?**\n",
    "  - While optimizing for **Recall** ensures we engage every potential churned customer, it can increase **False Positives**, leading to extra marketing costs.\n",
    "  - **F1-score balances Precision and Recall**, ensuring we prioritize retention without excessive resource wastage.\n",
    "\n",
    "## 3Ô∏è‚É£ Evaluating the Best Model on Different Test Splits\n",
    "- The best parameters from **GridSearchCV** are used to train and test models across different **test sizes (0.1, 0.2, 0.3, 0.4)**.\n",
    "- The performance of each model is compared using **Accuracy, Precision, Recall, F1-Score, and AUC-ROC** to analyze the impact of different test splits.\n",
    "\n",
    "üîπ **This process ensures that the model is well-optimized, achieves high recall, and generalizes effectively across different data splits.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç **Comparison of Logistic Regression and Decision Tree Models**\n",
    "\n",
    "## üìä **Performance Metrics**\n",
    "\n",
    "| üîπ **Metric**   | ‚ö° **Logistic Regression** | üå≥ **Decision Tree** |\n",
    "|---------------|---------------------|---------------|\n",
    "| **Accuracy**  | **0.521**           | **0.515**     |\n",
    "| **Precision** | **0.527**           | **0.521**     |\n",
    "| **Recall**    | **0.549**           | **0.574**     |\n",
    "| **F1-Score**  | **0.538**           | **0.546**     |\n",
    "| **AUC-ROC**   | **0.521**           | **0.515**     |\n",
    "\n",
    "---\n",
    "\n",
    "## üîé **Key Insights**\n",
    "\n",
    "### ‚úÖ **1. Logistic Regression Performs Slightly Better Overall**\n",
    "- Higher **Accuracy (0.521 vs. 0.515)** ‚Üí Slightly better at classifying churners and non-churners.\n",
    "- Higher **Precision (0.527 vs. 0.521)** ‚Üí Fewer false positives, meaning marketing efforts are more targeted.\n",
    "- Higher **AUC-ROC (0.521 vs. 0.515)** ‚Üí Marginally better at distinguishing churners from non-churners.\n",
    "\n",
    "### üåü **2. Decision Tree Excels in Recall (0.574 vs. 0.549)**\n",
    "- Captures **more actual churners** but misclassifies more non-churners.\n",
    "- If the **priority is customer retention at all costs**, the **Decision Tree is a better choice** despite lower precision.\n",
    "\n",
    "### ‚öñÔ∏è **3. F1-Scores Are Similar (0.538 vs. 0.546)**\n",
    "- Both models strike a similar balance between **precision and recall**, with Decision Tree being **slightly better**.\n",
    "\n",
    "---\n",
    "\n",
    "## üèÅ **Final Verdict**\n",
    "\n",
    "üîπ **üèÜ Logistic Regression is the best overall choice** due to its superior accuracy, precision, and AUC-ROC.\n",
    "üîπ **üå≥ Decision Tree is better if the goal is to capture as many churners as possible**, even if it means increased false positives.\n",
    "üîπ **Neither model is ideal**‚Äîthe **low AUC-ROC** suggests that **more advanced techniques** (feature engineering, hyperparameter tuning, ensemble models) are needed.\n",
    "\n",
    "üöÄ **Next Steps:**\n",
    "We will now explore **K-Nearest Neighbors and Ensemble Methods** to see if we can push performance further!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Assignment part 2"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors Classifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Define features and target\n",
    "x_knn = df_preprocessed.drop(['Churn', 'tenure'], axis=1)\n",
    "y_knn = df_preprocessed['Churn']\n",
    "\n",
    "# Define possible test sizes\n",
    "test_sizes = [0.1, 0.2, 0.3, 0.4]\n",
    "\n",
    "# Store results for each test size\n",
    "results_knn = []\n",
    "\n",
    "for test_size in test_sizes:\n",
    "    # Split the data\n",
    "    x_train_knn, x_test_knn, y_train_knn, y_test_knn = train_test_split(\n",
    "        x_knn, y_knn, test_size=test_size, random_state=42\n",
    "    )\n",
    "\n",
    "    # Initialize the KNN model with the specified hyperparameters (Hyper parameters are taken from the grid search)\n",
    "    knn_model = KNeighborsClassifier(\n",
    "        metric='euclidean',\n",
    "        n_neighbors=5,\n",
    "        weights='distance'\n",
    "    )\n",
    "\n",
    "    # Fit the model\n",
    "    knn_model.fit(x_train_knn, y_train_knn)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred_knn = knn_model.predict(x_test_knn)\n",
    "\n",
    "    # Evaluate performance\n",
    "    accuracy = accuracy_score(y_test_knn, y_pred_knn)\n",
    "    precision = precision_score(y_test_knn, y_pred_knn, zero_division=1)\n",
    "    recall = recall_score(y_test_knn, y_pred_knn, zero_division=1)\n",
    "    f1 = f1_score(y_test_knn, y_pred_knn, zero_division=1)\n",
    "    auc_roc = roc_auc_score(y_test_knn, y_pred_knn)\n",
    "\n",
    "    # Store the results\n",
    "    results_knn.append((test_size, accuracy, precision, recall, f1, auc_roc))\n",
    "\n",
    "# Display all results at the end\n",
    "print(\"\\nSummary of Results for K-Nearest Neighbors Classifier:\")\n",
    "for i, (test_size, accuracy, precision, recall, f1, auc_roc) in enumerate(results_knn):\n",
    "    if i == 1:  # Highlight the second record\n",
    "        print(\n",
    "            f\"\\033[1mTest Size: {test_size:.2f} | Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}, AUC-ROC: {auc_roc:.3f}\\033[0m\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"Test Size: {test_size:.2f} | Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}, AUC-ROC: {auc_roc:.3f}\")\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Feature matrix and target variable\n",
    "x_rf = df_preprocessed.drop(['Churn', 'tenure'], axis=1)\n",
    "y_rf = df_preprocessed['Churn']\n",
    "\n",
    "# Define possible test sizes\n",
    "test_sizes = [0.1, 0.2, 0.3, 0.4]\n",
    "\n",
    "# Store results for each test size\n",
    "results_rf = []\n",
    "\n",
    "for test_size in test_sizes:\n",
    "    # Split the data\n",
    "    x_train_rf, x_test_rf, y_train_rf, y_test_rf = train_test_split(\n",
    "        x_rf, y_rf, test_size=test_size, random_state=42\n",
    "    )\n",
    "\n",
    "    # Initialize Random Forest model with specified hyperparameters\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,  # Number of trees in the forest\n",
    "        max_depth=None,  # Maximum depth of the tree (None means nodes expand until all leaves are pure)\n",
    "        random_state=42,  # Random seed for reproducibility\n",
    "        bootstrap=True,  # Bagging enabled\n",
    "    )\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    rf_model.fit(x_train_rf, y_train_rf)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred_rf = rf_model.predict(x_test_rf)\n",
    "\n",
    "    # Evaluate performance\n",
    "    accuracy = accuracy_score(y_test_rf, y_pred_rf)\n",
    "    precision = precision_score(y_test_rf, y_pred_rf, zero_division=1)\n",
    "    recall = recall_score(y_test_rf, y_pred_rf, zero_division=1)\n",
    "    f1 = f1_score(y_test_rf, y_pred_rf, zero_division=1)\n",
    "    auc_roc = roc_auc_score(y_test_rf, y_pred_rf)\n",
    "\n",
    "    # Store results\n",
    "    results_rf.append((test_size, accuracy, precision, recall, f1, auc_roc))\n",
    "\n",
    "# Display all results at the end\n",
    "print(\"\\nSummary of Results for Random Forest Classifier (Bagging):\")\n",
    "for i, (test_size, accuracy, precision, recall, f1, auc_roc) in enumerate(results_rf):\n",
    "    if i == 1:  # Highlight the second record\n",
    "        print(\n",
    "            f\"\\033[1mTest Size: {test_size:.2f} | Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}, AUC-ROC: {auc_roc:.3f}\\033[0m\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"Test Size: {test_size:.2f} | Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}, AUC-ROC: {auc_roc:.3f}\"\n",
    "        )\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Data\n",
    "models = ['Decision Tree', 'Logistic Regression', 'KNN', 'Random Forest']\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n",
    "\n",
    "# Extract the second (index 1) results for each model and round to 3 decimal places\n",
    "data = {\n",
    "    'Decision Tree': [round(metric, 3) for metric in results_dt[1][1:]],  # Skip the test size (1st item)\n",
    "    'Logistic Regression': [round(metric, 3) for metric in results_lr[1][1:]],  # Skip the test size\n",
    "    'KNN': [round(metric, 3) for metric in results_knn[1][1:]],  # Skip the test size\n",
    "    'Random Forest': [round(metric, 3) for metric in results_rf[1][1:]]  # Skip the test size\n",
    "}\n",
    "\n",
    "# Print the created data dictionary\n",
    "# print(\"Extracted Data (Rounded to 3 Decimal Places):\")\n",
    "# print(data)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_models = pd.DataFrame(data, index=metrics)\n",
    "\n",
    "# Transform DataFrame into long format for Plotly\n",
    "df_melted = df_models.reset_index().melt(id_vars='index', var_name='Model', value_name='Score')\n",
    "df_melted.rename(columns={'index': 'Metric'}, inplace=True)\n",
    "\n",
    "# Plot using Plotly\n",
    "fig = px.histogram(\n",
    "    df_melted,\n",
    "    x='Metric',  # Metrics on the x-axis\n",
    "    y='Score',  # Scores on the y-axis\n",
    "    color='Model',  # Grouped by models\n",
    "    barmode='group',  # Bars grouped side-by-side\n",
    "    title='Model Performance Comparison',  # Title of the chart\n",
    "    color_discrete_sequence=px.colors.qualitative.Prism  # Define color palette\n",
    ")\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    xaxis_title='Evaluation Metrics',\n",
    "    yaxis_title='Score',\n",
    "    width=1000,\n",
    "    height=500,\n",
    "    legend_title='Models'\n",
    ")\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä **Customer Churn Prediction Model Evaluation**\n",
    "\n",
    "## üöÄ **Problem Statement**\n",
    "Predicting customer churn is crucial for **telecom companies** to retain customers and reduce revenue loss. Churn occurs when customers discontinue services, impacting business sustainability. **By accurately predicting churn, companies can implement targeted retention strategies** such as personalized offers, better customer service, and proactive engagement.\n",
    "\n",
    "---\n",
    "\n",
    "## üìâ **Overall Model Performance**\n",
    "All models exhibit **relatively low performance**, with accuracy scores hovering around **50%**. This suggests potential challenges in the dataset, such as:\n",
    "üîπ **High noise** ‚Äì irrelevant or inconsistent data\n",
    "üîπ **Weak predictive features** ‚Äì limited strong indicators of churn\n",
    "\n",
    "However, even slight improvements over **random guessing (50%)** can translate into **significant business impact**, making these insights valuable for retention efforts.\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ñ **Models Evaluated & Metrics**\n",
    "We trained and tested **four machine learning models** to predict churn:\n",
    "\n",
    "‚úî **Decision Tree**\n",
    "‚úî **Logistic Regression**\n",
    "‚úî **K-Nearest Neighbors (KNN)**\n",
    "‚úî **Random Forest**\n",
    "\n",
    "Each model was evaluated using:\n",
    "\n",
    "- **Accuracy** ‚Üí Overall correctness of the model.\n",
    "- **Precision** ‚Üí Percentage of predicted churners that actually churned.\n",
    "- **Recall** ‚Üí Percentage of actual churners correctly identified.\n",
    "- **F1-Score** ‚Üí Balances precision and recall for overall effectiveness.\n",
    "- **AUC-ROC** ‚Üí Measures the model‚Äôs ability to distinguish churners from non-churners.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä **Performance Comparison**\n",
    "\n",
    "| Model                   | Accuracy  | Precision | Recall    | F1-Score  | AUC-ROC   |\n",
    "|-------------------------|-----------|-----------|-----------|-----------|-----------|\n",
    "| **Decision Tree**       | **0.516** | 0.521     | **0.574** | **0.546** | 0.515     |\n",
    "| **Logistic Regression** | **0.522** | **0.527** | 0.549     | 0.538     | **0.521** |\n",
    "| **KNN**                 | 0.508     | 0.516     | 0.502     | 0.509     | 0.508     |\n",
    "| **Random Forest**       | 0.504     | 0.512     | 0.505     | 0.509     | 0.504     |\n",
    "\n",
    "---\n",
    "\n",
    "## üèÜ **Best Model Selection ‚Äì Logistic Regression**\n",
    "Despite all models performing similarly, **Logistic Regression is the most suitable choice** due to its slightly higher performance across key metrics.\n",
    "\n",
    "### üîπ **Why Logistic Regression?**\n",
    "\n",
    "‚úÖ **Highest Accuracy (0.522) and AUC-ROC (0.521)**\n",
    "‚úî Correctly classifies the most instances overall.\n",
    "‚úî Better at distinguishing churners vs. non-churners than the other models.\n",
    "\n",
    "‚úÖ **Balanced Recall (0.549) and Precision (0.527)**\n",
    "‚úî Decision Tree has a **higher recall (0.574)** but also a **higher false positive rate**, meaning more unnecessary customer interventions.\n",
    "‚úî Logistic Regression **strikes a better balance**, reducing wasted retention efforts while still identifying churners effectively.\n",
    "\n",
    "‚úÖ **More Stable & Interpretable**\n",
    "‚úî Logistic Regression is **computationally efficient**, easy to interpret, and less prone to **overfitting** than Decision Trees.\n",
    "‚úî Businesses can **easily trust and explain** its predictions for actionable insights.\n",
    "\n",
    "---\n",
    "\n",
    "## üí° **Business Impact**\n",
    "Churn prediction is a **trade-off between recall and precision**:\n",
    "\n",
    "üîπ **Decision Tree prioritizes recall**, meaning **it catches more churners but misclassifies more loyal customers**, increasing unnecessary interventions.\n",
    "üîπ **Logistic Regression balances both recall and precision**, reducing false positives while still identifying potential churners.\n",
    "üîπ **Even a 1% improvement in churn prediction could translate to significant annual savings** in retention costs.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÆ **Conclusion**\n",
    "For **customer churn prediction**, **Logistic Regression emerges as the best model** due to its **superior accuracy, interpretability, and balance between recall & precision**. While improvements are needed, this model provides **a solid foundation** for real-world deployment in telecom retention strategies.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
